# 분산 시스템과 etcd 심화

> *"분산 시스템은 당신이 존재조차 몰랐던 컴퓨터의 장애로 인해 
> 당신의 컴퓨터가 사용 불가능해지는 시스템이다."*
> — Leslie Lamport (2013년 튜링상 수상자, Paxos 발명자)

단일 서버로 모든 것을 처리하던 시대는 끝났습니다. 현대 시스템은 수십, 수백 대의 서버에 걸쳐 동작합니다. 그러나 여러 컴퓨터가 네트워크로 연결되는 순간, 우리는 완전히 새로운 유형의 문제와 마주합니다: 네트워크는 언제든 끊어질 수 있고, 서버는 예고 없이 죽을 수 있으며, 시계는 서로 다르게 흐릅니다.

Kubernetes 클러스터의 모든 상태는 etcd라는 분산 키-값 저장소에 저장됩니다. etcd가 일관성을 잃으면 클러스터 전체가 혼란에 빠집니다. Pod가 중복 생성되거나, 이미 삭제된 리소스가 다시 나타나거나, 설정이 롤백됩니다. 이것이 분산 시스템의 합의(Consensus) 문제를 이해해야 하는 이유입니다.

이 장에서는 분산 시스템의 근본적인 도전과 해결책을 탐구합니다. CAP 정리가 왜 중요한지, Raft 알고리즘이 어떻게 여러 노드의 합의를 이끌어내는지, etcd가 이를 어떻게 구현하는지 깊이 있게 다룹니다.

---

## 이 장에서 다루는 내용

이 장을 읽고 나면 다음을 이해할 수 있습니다:

- **분산 시스템의 근본적 도전**: 네트워크 분할, 부분 장애, 시간 동기화 문제
- **CAP 정리**: 일관성, 가용성, 분할 내성의 트레이드오프
- **합의 문제**: 여러 노드가 하나의 값에 동의하는 것이 왜 어려운가
- **Raft 알고리즘**: 리더 선출, 로그 복제, 안전성 보장
- **etcd 아키텍처**: Raft 기반 분산 키-값 저장소의 내부 구조
- **쿼럼과 가용성**: 노드 수와 장애 허용의 관계
- **장애 시나리오**: 실제 장애 상황과 복구 방법

> **📘 개념 (Concept)**: 분산 시스템을 이해하지 않고 Kubernetes를 운영하는 것은 엔진 작동 원리를 모르고 자동차를 정비하는 것과 같습니다. 평소에는 괜찮지만, 문제가 발생했을 때 근본 원인을 찾지 못합니다.

---

## 1. 분산 시스템의 도전

### 1.1. 왜 분산 시스템인가?

단일 서버의 한계를 넘기 위해 분산 시스템을 사용합니다:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      단일 서버 vs 분산 시스템                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   단일 서버 (Monolithic)                                                    │
│   ─────────────────────────                                                 │
│   ┌─────────────────────────────────────────┐                              │
│   │              Single Server               │                              │
│   │                                          │                              │
│   │  장점:                                   │                              │
│   │  • 단순함 (공유 상태, 단일 시계)         │                              │
│   │  • 강한 일관성 (즉시 반영)               │                              │
│   │  • 트랜잭션 용이                         │                              │
│   │                                          │                              │
│   │  한계:                                   │                              │
│   │  • 수직 확장 한계 (CPU, 메모리)          │                              │
│   │  • 단일 장애점 (SPOF)                    │                              │
│   │  • 물리적 한계                           │                              │
│   │                                          │                              │
│   └─────────────────────────────────────────┘                              │
│                                                                             │
│   분산 시스템 (Distributed)                                                 │
│   ─────────────────────────────                                             │
│   ┌─────────────┐   ┌─────────────┐   ┌─────────────┐                      │
│   │   Node 1    │◄─►│   Node 2    │◄─►│   Node 3    │                      │
│   │             │   │             │   │             │                      │
│   └─────────────┘   └─────────────┘   └─────────────┘                      │
│          ▲                 ▲                 ▲                              │
│          │                 │                 │                              │
│          └─────────────────┴─────────────────┘                              │
│                        Network                                              │
│                                                                             │
│   장점:                                                                     │
│   • 수평 확장 (노드 추가)                                                   │
│   • 고가용성 (노드 장애 허용)                                               │
│   • 지리적 분산 가능                                                        │
│                                                                             │
│   도전:                                                                     │
│   • 네트워크 장애/지연                                                      │
│   • 데이터 일관성 유지                                                      │
│   • 동시성 제어                                                             │
│   • 부분 장애 처리                                                          │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 1.2. 분산 시스템의 세 가지 악몽

분산 시스템에서는 단일 서버에서 당연하게 여겼던 것들이 더 이상 당연하지 않습니다:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      분산 시스템의 근본적 도전                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   1. 네트워크는 신뢰할 수 없다                                               │
│   ─────────────────────────────                                             │
│                                                                             │
│   Node A ─────────?─────────► Node B                                       │
│                                                                             │
│   • 메시지가 도착하지 않을 수 있음                                           │
│   • 메시지가 중복 도착할 수 있음                                             │
│   • 메시지 순서가 바뀔 수 있음                                               │
│   • 메시지가 임의로 지연될 수 있음                                           │
│                                                                             │
│   결과: "Node B가 응답 안함" = 죽은 것인가? 느린 것인가? 네트워크 문제인가?  │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   2. 시계는 동기화되지 않는다                                                │
│   ─────────────────────────────                                             │
│                                                                             │
│   Node A 시계: 10:00:00.000                                                │
│   Node B 시계: 10:00:00.150   ← 150ms 차이                                  │
│   Node C 시계: 09:59:59.950   ← 50ms 뒤처짐                                 │
│                                                                             │
│   문제 상황:                                                                │
│   • Node A가 10:00:00에 "x=1" 기록                                          │
│   • Node C가 09:59:59.980에 "x=2" 기록                                      │
│   • 실제로는 C가 나중에 기록했는데, 타임스탬프는 더 이전                      │
│                                                                             │
│   결과: 타임스탬프로 순서를 정할 수 없음 → Logical Clock (Lamport) 필요      │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   3. 부분 장애 (Partial Failure)                                            │
│   ─────────────────────────────                                             │
│                                                                             │
│   단일 서버: 동작 또는 완전 정지 (명확)                                      │
│                                                                             │
│   분산 시스템:                                                              │
│   ┌─────────────┐   ┌─────────────┐   ┌─────────────┐                      │
│   │   Node A    │   │   Node B    │   │   Node C    │                      │
│   │   (정상)    │   │   (느림)    │   │   (죽음)    │                      │
│   └─────────────┘   └─────────────┘   └─────────────┘                      │
│                                                                             │
│   문제:                                                                     │
│   • 일부만 장애인 상태에서 시스템은 어떻게 동작해야 하는가?                   │
│   • 느린 노드를 죽은 것으로 판단해야 하는가?                                  │
│   • 네트워크 분할 시 어느 쪽이 "진짜" 클러스터인가?                           │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

> **⚠️ 주의 (Warning)**: "네트워크는 안정적이다", "지연 시간은 0이다", "시계는 동기화되어 있다"는 분산 시스템의 대표적인 오류(Fallacies)입니다. 이런 가정을 하면 프로덕션에서 반드시 문제가 발생합니다.

### 1.3. 비잔틴 장애 vs 비-비잔틴 장애

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         장애 유형 분류                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Crash Failure (비-비잔틴, 단순 장애)                                      │
│   ──────────────────────────────────────                                    │
│   • 노드가 멈추거나 응답하지 않음                                            │
│   • 잘못된 데이터를 보내지는 않음                                            │
│   • 예측 가능한 장애                                                        │
│                                                                             │
│   [정상 동작] ──► [장애 발생] ──► [응답 없음]                               │
│                                                                             │
│   예: 서버 전원 꺼짐, 프로세스 크래시, 네트워크 단절                         │
│   대응: Raft, Paxos (쿼럼 기반 합의)                                        │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Byzantine Failure (비잔틴 장애)                                           │
│   ─────────────────────────────────                                         │
│   • 노드가 임의의 행동을 할 수 있음                                          │
│   • 거짓 데이터를 보낼 수 있음                                               │
│   • 악의적이거나 버그로 인한 예측 불가 행동                                   │
│                                                                             │
│   [정상 동작] ──► [장애 발생] ──► [거짓말/이상한 응답]                       │
│                                                                             │
│   예: 해킹된 노드, 심각한 소프트웨어 버그                                    │
│   대응: PBFT, Tendermint (3f+1 노드 필요, f는 악의적 노드 수)                │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Kubernetes/etcd: 비-비잔틴 장애만 가정                                    │
│                                                                             │
│   이유:                                                                     │
│   • 클러스터 내부 노드는 신뢰할 수 있다고 가정                                │
│   • 비잔틴 장애 대응은 성능 오버헤드가 큼                                    │
│   • 보안은 네트워크 수준(mTLS)에서 처리                                      │
│                                                                             │
│   주의: etcd 노드가 해킹되면 클러스터 전체가 위험                             │
│   → etcd 접근 제한 및 암호화 필수                                           │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 섹션 요약

- **분산 시스템의 필요성**: 확장성, 가용성, 내결함성
- **핵심 도전**: 네트워크 불안정, 시계 비동기화, 부분 장애
- **장애 유형**: Crash 장애(etcd/Raft 대응) vs Byzantine 장애(블록체인 대응)

---

## 2. CAP 정리

### 2.1. CAP 정리의 이해

2000년 Eric Brewer가 제안하고 2002년 증명된 CAP 정리는 분산 시스템의 근본적인 트레이드오프를 설명합니다:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           CAP 정리 상세                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   CAP: 분산 시스템은 다음 세 가지 중 최대 두 가지만 동시에 보장할 수 있다    │
│                                                                             │
│                        Consistency (일관성)                                 │
│                        ────────────────────                                 │
│                        모든 노드가 같은 시점에                               │
│                        같은 데이터를 본다                                    │
│                                                                             │
│                              /\                                             │
│                             /  \                                            │
│                            /    \                                           │
│                           / CA   \                                          │
│                          /  영역  \     ← 네트워크 분할이 없을 때만 가능      │
│                         /          \                                        │
│                        /     |      \                                       │
│                       /      |       \                                      │
│                      /   CP  |  AP    \                                     │
│                     /        |         \                                    │
│                    ──────────┴──────────                                    │
│         Partition              Availability                                 │
│         Tolerance              (가용성)                                     │
│         (분할 내성)            ─────────────                                 │
│         ────────────           항상 응답을 반환                              │
│         네트워크 분할에도       (최신 데이터가                               │
│         시스템이 계속 동작      아닐 수 있음)                                │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   현실: 네트워크 분할(P)은 반드시 발생한다                                   │
│   ───────────────────────────────────────                                   │
│                                                                             │
│   분산 시스템에서 네트워크 분할은 "발생할 수 있는 것"이 아니라               │
│   "반드시 발생하는 것"입니다. 따라서 P는 필수입니다.                         │
│                                                                             │
│   실제 선택: CP vs AP                                                       │
│                                                                             │
│   ┌────────────────────────────┬────────────────────────────┐              │
│   │         CP 시스템          │         AP 시스템          │              │
│   │   (일관성 우선)            │   (가용성 우선)            │              │
│   ├────────────────────────────┼────────────────────────────┤              │
│   │ 네트워크 분할 시           │ 네트워크 분할 시           │              │
│   │ → 쓰기 거부 (에러 반환)    │ → 쓰기 허용 (나중에 병합)  │              │
│   │                            │                            │              │
│   │ 장점: 데이터 일관성 보장   │ 장점: 항상 응답 가능       │              │
│   │ 단점: 일시적 불가용        │ 단점: 충돌 해결 필요       │              │
│   │                            │                            │              │
│   │ 예: etcd, Consul, ZooKeeper│ 예: Cassandra, DynamoDB    │              │
│   │     HBase, Spanner         │     CouchDB, Riak          │              │
│   │                            │                            │              │
│   │ Kubernetes 선택: CP        │                            │              │
│   └────────────────────────────┴────────────────────────────┘              │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

> **📘 개념 (Concept)**: Kubernetes가 CP를 선택한 이유는 명확합니다. Pod가 두 번 생성되거나, 이미 삭제된 Service가 다시 나타나는 것보다, 일시적으로 API가 응답하지 않는 것이 훨씬 낫습니다. 데이터 불일치는 시스템 전체를 혼란에 빠뜨립니다.


### 2.2. CAP 정리의 실제 의미

CAP 정리는 종종 오해됩니다. 실제로는 더 미묘한 의미를 가집니다:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        CAP 정리의 올바른 이해                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   오해 1: "CP 시스템은 항상 가용하지 않다"                                   │
│   ───────────────────────────────────────                                   │
│                                                                             │
│   틀림: CP 시스템도 네트워크가 정상일 때는 완전히 가용합니다.                │
│   CAP는 "네트워크 분할이 발생했을 때"의 트레이드오프입니다.                   │
│                                                                             │
│   etcd (CP):                                                                │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │  정상 상태: C + A (일관성 + 가용성)                                  │  │
│   │  네트워크 분할 시: C 선택 (쿼럼 없으면 쓰기 거부)                     │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   오해 2: "CAP는 이분법적 선택이다"                                         │
│   ───────────────────────────────────                                       │
│                                                                             │
│   틀림: 실제로는 스펙트럼입니다.                                            │
│                                                                             │
│   강한 일관성 ◄──────────────────────────────────────────► 높은 가용성     │
│      (CP)                                                      (AP)        │
│                                                                             │
│   • 동기식 복제 (모든 노드 확인 후 응답)                                     │
│   • 쿼럼 기반 (과반수 확인 후 응답) ← etcd                                  │
│   • 비동기식 복제 (즉시 응답, 나중에 복제)                                   │
│   • 최종 일관성 (언젠가 일치)                                               │
│                                                                             │
│   오해 3: "CA 시스템이 가능하다"                                            │
│   ───────────────────────────────                                           │
│                                                                             │
│   기술적으로 틀림: 단일 서버는 CA이지만, "분산 시스템"이 아닙니다.           │
│   분산 시스템에서 네트워크 분할은 피할 수 없으므로 P는 필수입니다.           │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 2.3. 일관성 모델

CAP의 Consistency는 여러 수준이 있습니다:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          일관성 모델 스펙트럼                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   강한 일관성 ◄─────────────────────────────────────────────► 약한 일관성  │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │                                                                       │  │
│   │   Linearizability (선형화 가능성) - 가장 강함                        │  │
│   │   ──────────────────────────────────────────                         │  │
│   │   • 모든 연산이 실시간 순서대로 적용된 것처럼 보임                    │  │
│   │   • 쓰기 완료 후 모든 읽기는 그 값을 반환                             │  │
│   │   • etcd의 기본 모드                                                 │  │
│   │                                                                       │  │
│   │   예시:                                                               │  │
│   │   T1: Client A writes x=1 (완료)                                     │  │
│   │   T2: Client B reads x → 반드시 1                                    │  │
│   │                                                                       │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                        │                                                    │
│                        ▼                                                    │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │                                                                       │  │
│   │   Sequential Consistency (순차 일관성)                               │  │
│   │   ─────────────────────────────────────                              │  │
│   │   • 모든 프로세스가 같은 순서를 본다                                  │  │
│   │   • 실시간 순서와 다를 수 있음                                        │  │
│   │                                                                       │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                        │                                                    │
│                        ▼                                                    │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │                                                                       │  │
│   │   Causal Consistency (인과 일관성)                                   │  │
│   │   ─────────────────────────────────                                  │  │
│   │   • 인과 관계가 있는 연산만 순서 보장                                 │  │
│   │   • 독립적 연산은 다른 순서로 보일 수 있음                            │  │
│   │                                                                       │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                        │                                                    │
│                        ▼                                                    │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │                                                                       │  │
│   │   Eventual Consistency (최종 일관성) - 가장 약함                     │  │
│   │   ──────────────────────────────────────────────                     │  │
│   │   • 업데이트가 멈추면 "언젠가" 모든 노드가 같은 값                    │  │
│   │   • 그 전까지는 다른 값을 볼 수 있음                                  │  │
│   │   • DNS, CDN, 캐시 등에서 사용                                       │  │
│   │                                                                       │  │
│   │   예시:                                                               │  │
│   │   T1: Client A writes x=1                                            │  │
│   │   T2: Client B reads x → 이전 값 또는 1 (불확실)                     │  │
│   │   T3: (시간 경과 후) Client B reads x → 1                            │  │
│   │                                                                       │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   Kubernetes/etcd: Linearizability (가장 강한 일관성)                       │
│   이유: 클러스터 상태가 불일치하면 Pod 중복 생성 등 심각한 문제 발생         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 섹션 요약

- **CAP 정리**: C, A, P 중 두 가지만 선택 가능
- **P는 필수**: 네트워크 분할은 피할 수 없음 → CP vs AP 선택
- **Kubernetes/etcd**: CP 선택 (일관성 우선)
- **일관성 수준**: Linearizability (가장 강함) 사용

---

## 3. 합의 알고리즘

### 3.1. 합의 문제란?

합의(Consensus)는 분산 시스템의 핵심 문제입니다:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                            합의 문제 정의                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   문제: N개의 노드가 하나의 값에 동의해야 한다                               │
│                                                                             │
│   시나리오: 3개 노드가 "다음 리더가 누구인가"에 동의                         │
│                                                                             │
│   초기 상태:                                                                │
│   ┌─────────────┐   ┌─────────────┐   ┌─────────────┐                      │
│   │   Node A    │   │   Node B    │   │   Node C    │                      │
│   │   제안: A   │   │   제안: B   │   │   제안: A   │                      │
│   └─────────────┘   └─────────────┘   └─────────────┘                      │
│                                                                             │
│   합의 후:                                                                  │
│   ┌─────────────┐   ┌─────────────┐   ┌─────────────┐                      │
│   │   Node A    │   │   Node B    │   │   Node C    │                      │
│   │   결정: A   │   │   결정: A   │   │   결정: A   │                      │
│   └─────────────┘   └─────────────┘   └─────────────┘                      │
│                     모든 노드가 "A"에 동의                                  │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   합의의 속성 (반드시 만족해야 함)                                          │
│   ─────────────────────────────────                                         │
│                                                                             │
│   1. Agreement (동의)                                                       │
│      모든 정상 노드는 같은 값을 결정한다                                     │
│                                                                             │
│   2. Validity (유효성)                                                      │
│      결정된 값은 어떤 노드가 제안한 값이다                                   │
│      (임의의 값이 아님)                                                     │
│                                                                             │
│   3. Termination (종료)                                                     │
│      모든 정상 노드는 결국 결정에 도달한다                                   │
│      (무한 대기 없음)                                                       │
│                                                                             │
│   4. Integrity (무결성)                                                     │
│      한 번 결정되면 바뀌지 않는다                                            │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   왜 어려운가?                                                              │
│   ────────────                                                              │
│                                                                             │
│   • 노드가 죽을 수 있음 (누가 죽었는지 모름)                                 │
│   • 네트워크 지연으로 메시지 순서 불확실                                     │
│   • 동시에 여러 값이 제안될 수 있음                                          │
│   • "다수결"로도 해결 안 됨 (투표 도중 노드 장애 가능)                       │
│                                                                             │
│   FLP 불가능성 정리 (Fischer, Lynch, Paterson, 1985):                       │
│   "비동기 시스템에서 단 하나의 노드 장애만으로도 합의 보장 불가능"            │
│                                                                             │
│   해결책: 타임아웃 기반 부분 동기 모델 (Raft, Paxos)                         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 3.2. 주요 합의 알고리즘 비교

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                       합의 알고리즘 비교                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │ 알고리즘 │ 연도 │ 장애 유형 │ 복잡도 │ 특징              │ 사용 예   │  │
│   ├─────────────────────────────────────────────────────────────────────┤  │
│   │ Paxos   │ 1989│ Crash   │ 높음  │ 최초의 실용적 합의  │ Chubby,  │  │
│   │         │     │         │       │ 알고리즘, 이해 어려움│ Megastore│  │
│   ├─────────────────────────────────────────────────────────────────────┤  │
│   │ Raft    │ 2014│ Crash   │ 중간  │ 이해하기 쉽게 설계  │ etcd,    │  │
│   │         │     │         │       │ 리더 기반           │ Consul   │  │
│   ├─────────────────────────────────────────────────────────────────────┤  │
│   │ ZAB     │ 2008│ Crash   │ 중간  │ ZooKeeper 전용      │ ZooKeeper│  │
│   │         │     │         │       │ Paxos 변형          │          │  │
│   ├─────────────────────────────────────────────────────────────────────┤  │
│   │ PBFT    │ 1999│Byzantine│ 높음  │ 악의적 노드 허용    │Hyperledge│  │
│   │         │     │         │       │ 3f+1 노드 필요      │          │  │
│   ├─────────────────────────────────────────────────────────────────────┤  │
│   │Tendermint│2014│Byzantine│ 중간  │ 블록체인 최적화     │ Cosmos   │  │
│   │         │     │         │       │ 빠른 최종성         │          │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   etcd/Kubernetes: Raft 선택                                               │
│   이유:                                                                     │
│   • 이해하기 쉬움 (운영, 디버깅 용이)                                       │
│   • 클러스터 내부는 신뢰할 수 있음 (Byzantine 불필요)                        │
│   • 리더 기반으로 구현이 단순                                               │
│   • 충분히 검증됨 (2014년 이후 많은 프로덕션 사용)                           │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

> **💡 팁 (Tip)**: Paxos와 Raft는 이론적으로 동등한 문제를 해결하지만, Raft는 "이해 가능성"을 목표로 설계되었습니다. 논문 제목 자체가 "In Search of an Understandable Consensus Algorithm"입니다.

### 섹션 요약

- **합의 문제**: 여러 노드가 하나의 값에 동의
- **속성**: Agreement, Validity, Termination, Integrity
- **어려운 이유**: 비동기 환경에서 장애와 동시성
- **Raft**: etcd가 사용하는 이해하기 쉬운 합의 알고리즘

---

## 4. Raft 알고리즘 심화

### 4.1. Raft 개요

Raft는 2014년 Stanford의 Diego Ongaro와 John Ousterhout가 발표한 합의 알고리즘입니다. "이해 가능성"을 최우선 목표로 설계되었습니다.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          Raft의 세 가지 핵심 요소                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Raft는 합의 문제를 세 가지 독립적인 하위 문제로 분해합니다:                 │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │                                                                       │  │
│   │   1. Leader Election (리더 선출)                                     │  │
│   │      ────────────────────────────                                    │  │
│   │      • 기존 리더가 죽으면 새 리더 선출                                │  │
│   │      • 한 번에 하나의 리더만 존재                                     │  │
│   │      • 랜덤 타임아웃으로 동시 후보 방지                               │  │
│   │                                                                       │  │
│   │   2. Log Replication (로그 복제)                                     │  │
│   │      ──────────────────────────                                      │  │
│   │      • 리더가 클라이언트 요청을 로그에 기록                           │  │
│   │      • 리더가 Follower들에게 로그 복제                                │  │
│   │      • 과반수 복제 시 "커밋"                                          │  │
│   │                                                                       │  │
│   │   3. Safety (안전성)                                                 │  │
│   │      ──────────────                                                  │  │
│   │      • 커밋된 로그는 절대 덮어쓰이지 않음                             │  │
│   │      • 모든 노드가 같은 로그 순서를 유지                              │  │
│   │      • 새 리더는 모든 커밋된 로그를 가짐                              │  │
│   │                                                                       │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 4.2. 노드 상태와 Term

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         Raft 노드 상태 전이                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   모든 Raft 노드는 세 가지 상태 중 하나입니다:                               │
│                                                                             │
│   ┌───────────────────────────────────────────────────────────────────────┐│
│   │                                                                       ││
│   │                    ┌──────────────────┐                               ││
│   │                    │    Follower      │◄──────────────────────────┐  ││
│   │                    │                  │                           │  ││
│   │                    │ • 수동적 역할    │                           │  ││
│   │                    │ • 리더의 요청에  │                           │  ││
│   │                    │   응답만 함      │                           │  ││
│   │                    │ • 선거 타임아웃  │                           │  ││
│   │                    │   대기 중        │                           │  ││
│   │                    └────────┬─────────┘                           │  ││
│   │                             │                                      │  ││
│   │                             │ 선거 타임아웃                         │  ││
│   │                             │ (리더 heartbeat 없음)                 │  ││
│   │                             ▼                                      │  ││
│   │                    ┌──────────────────┐                           │  ││
│   │     새 리더 발견   │    Candidate     │    선거 패배/동률          │  ││
│   │     또는 더 높은   │                  │    또는 새 term 발견       │  ││
│   │     term 발견      │ • 선거 시작      │────────────────────────────┘  ││
│   │         │          │ • 자신에게 투표  │                               ││
│   │         │          │ • 다른 노드에    │                               ││
│   │         │          │   투표 요청      │                               ││
│   │         │          └────────┬─────────┘                               ││
│   │         │                   │                                         ││
│   │         │                   │ 과반수 득표                              ││
│   │         │                   ▼                                         ││
│   │         │          ┌──────────────────┐                               ││
│   │         └──────────│     Leader       │                               ││
│   │                    │                  │                               ││
│   │                    │ • 클라이언트 요청│                               ││
│   │                    │   처리           │                               ││
│   │                    │ • 로그 복제      │                               ││
│   │                    │ • heartbeat 전송 │                               ││
│   │                    └──────────────────┘                               ││
│   │                                                                       ││
│   └───────────────────────────────────────────────────────────────────────┘│
│                                                                             │
│   Term (임기): 논리적 시간 단위                                             │
│   ───────────────────────────────                                           │
│                                                                             │
│   ┌────────────────────────────────────────────────────────────────────────┐│
│   │ Term 1      │ Term 2          │ Term 3      │ Term 4                  ││
│   │             │                 │             │                          ││
│   │ Leader: A   │ (선거, 무승부)  │ Leader: B   │ Leader: C               ││
│   │ ───────────►│ ───────────────►│ ───────────►│ ───────────────►        ││
│   │             │                 │             │                          ││
│   │ A 정상 운영 │ A 장애, B,C 동시│ B 당선      │ B 장애, C 당선          ││
│   │             │ 후보, 재선거    │             │                          ││
│   └────────────────────────────────────────────────────────────────────────┘│
│                                                                             │
│   Term 규칙:                                                                │
│   • 각 Term에는 최대 한 명의 리더                                           │
│   • 노드는 자신보다 높은 Term의 메시지를 받으면 즉시 Follower로 전환        │
│   • Term이 낮은 메시지는 무시                                               │
│   • 선거 시작 시 Term 증가                                                  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```


### 4.3. 리더 선출 (Leader Election)

리더 선출은 Raft의 핵심 메커니즘입니다:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        Raft 리더 선출 상세 과정                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   시나리오: 3노드 클러스터, Node A(리더) 장애 발생                           │
│                                                                             │
│   시간 ────────────────────────────────────────────────────────────────►    │
│                                                                             │
│   1. 정상 상태 (Term 1)                                                     │
│   ─────────────────────                                                     │
│   ┌─────────────────┐   ┌─────────────────┐   ┌─────────────────┐          │
│   │   Node A        │   │   Node B        │   │   Node C        │          │
│   │   Leader        │──►│   Follower      │   │   Follower      │          │
│   │   Term: 1       │   │   Term: 1       │   │   Term: 1       │          │
│   │                 │──►│                 │──►│                 │          │
│   │  heartbeat 전송 │   │ heartbeat 수신  │   │ heartbeat 수신  │          │
│   └─────────────────┘   └─────────────────┘   └─────────────────┘          │
│          │                                                                  │
│          X 장애 발생 (전원 꺼짐, 네트워크 단절 등)                           │
│          │                                                                  │
│   2. 선거 타임아웃 (150-300ms 랜덤)                                         │
│   ─────────────────────────────────                                         │
│   ┌─────────────────┐   ┌─────────────────┐   ┌─────────────────┐          │
│   │   Node A        │   │   Node B        │   │   Node C        │          │
│   │   (Down)        │   │   Follower      │   │   Follower      │          │
│   │                 │   │   Term: 1       │   │   Term: 1       │          │
│   │                 │   │                 │   │                 │          │
│   │                 │   │  타임아웃 대기  │   │  타임아웃 대기  │          │
│   │                 │   │  180ms         │   │  250ms         │          │
│   └─────────────────┘   └─────────────────┘   └─────────────────┘          │
│                                │                                            │
│                                │ B가 먼저 타임아웃                           │
│                                ▼                                            │
│   3. Candidate로 전환 및 투표 요청                                          │
│   ────────────────────────────────                                          │
│   ┌─────────────────┐   ┌─────────────────┐   ┌─────────────────┐          │
│   │   Node A        │   │   Node B        │   │   Node C        │          │
│   │   (Down)        │   │   Candidate     │   │   Follower      │          │
│   │                 │   │   Term: 2       │   │   Term: 1       │          │
│   │                 │   │                 │   │                 │          │
│   │                 │   │ 1. Term++ (2)   │   │                 │          │
│   │                 │   │ 2. 자신에게 투표│   │                 │          │
│   │                 │   │ 3. RequestVote  │   │                 │          │
│   │                 │   │    RPC 전송     │──►│                 │          │
│   └─────────────────┘   └─────────────────┘   └─────────────────┘          │
│                                                        │                    │
│   4. 투표 응답                                         │                    │
│   ─────────────                                        ▼                    │
│   ┌─────────────────┐   ┌─────────────────┐   ┌─────────────────┐          │
│   │   Node A        │   │   Node B        │◄──│   Node C        │          │
│   │   (Down)        │   │   Candidate     │   │   Follower      │          │
│   │                 │   │   Term: 2       │   │   Term: 2       │          │
│   │                 │   │                 │   │                 │          │
│   │                 │   │ 득표: 2/3       │   │ VoteGranted     │          │
│   │                 │   │ (자신 + C)      │   │ (B에게 투표)    │          │
│   │                 │   │                 │   │                 │          │
│   │                 │   │ 과반수 달성!    │   │ Term 업데이트   │          │
│   └─────────────────┘   └─────────────────┘   └─────────────────┘          │
│                                │                                            │
│                                ▼                                            │
│   5. 새 리더 확립 (Term 2)                                                  │
│   ─────────────────────────                                                 │
│   ┌─────────────────┐   ┌─────────────────┐   ┌─────────────────┐          │
│   │   Node A        │   │   Node B        │   │   Node C        │          │
│   │   (Down)        │   │   Leader ★      │──►│   Follower      │          │
│   │                 │   │   Term: 2       │   │   Term: 2       │          │
│   │                 │   │                 │   │                 │          │
│   │                 │   │ heartbeat 시작  │   │ heartbeat 수신  │          │
│   │                 │   │                 │   │                 │          │
│   └─────────────────┘   └─────────────────┘   └─────────────────┘          │
│                                                                             │
│   선거 규칙:                                                                │
│   ─────────────                                                             │
│   • 각 Term에 각 노드는 최대 한 번만 투표                                   │
│   • 먼저 도착한 RequestVote에만 투표                                        │
│   • 후보자의 로그가 자신보다 최신이어야 투표                                 │
│     (안전성 보장: 최신 로그를 가진 노드만 리더 가능)                          │
│   • 랜덤 타임아웃 (150-300ms)으로 동시 후보 방지                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

> **🔍 심화 학습 (Deep Dive)**: 선거 타임아웃이 랜덤인 이유는 "split vote"를 방지하기 위해서입니다. 모든 노드가 동시에 후보가 되면 투표가 분산되어 아무도 과반을 얻지 못합니다. 랜덤 타임아웃으로 한 노드가 먼저 선거를 시작하면 대부분 그 노드가 당선됩니다.

### 4.4. 로그 복제 (Log Replication)

리더가 클라이언트 요청을 처리하고 Follower들에게 복제하는 과정입니다:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        Raft 로그 복제 과정                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   클라이언트: "x = 5"                                                       │
│                                                                             │
│   1. 클라이언트 → 리더: 쓰기 요청                                           │
│   ────────────────────────────────                                          │
│                                                                             │
│   Client ──────────────────► Leader (Node B)                               │
│          PUT x=5                                                            │
│                                                                             │
│   2. 리더: 로그에 엔트리 추가 (uncommitted)                                 │
│   ───────────────────────────────────────                                   │
│                                                                             │
│   Leader Log:                                                               │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │ Index │ Term │ Command │ Status      │                               │  │
│   ├─────────────────────────────────────────────────────────────────────┤  │
│   │   1   │  1   │ x=1     │ committed   │                               │  │
│   │   2   │  1   │ y=2     │ committed   │                               │  │
│   │   3   │  2   │ x=5     │ uncommitted │ ◄── 새 엔트리                  │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   3. 리더 → Followers: AppendEntries RPC                                   │
│   ─────────────────────────────────────                                     │
│                                                                             │
│   Leader                  Follower 1              Follower 2                │
│   ┌──────────┐            ┌──────────┐            ┌──────────┐             │
│   │ [1][2][3]│───────────►│ [1][2]   │            │ [1][2]   │             │
│   │          │AppendEntries│          │            │          │             │
│   │          │ prevLogIndex=2         │            │          │             │
│   │          │ prevLogTerm=1          │            │          │             │
│   │          │ entries=[{3,2,x=5}]    │            │          │             │
│   │          │ leaderCommit=2         │            │          │             │
│   └──────────┘            └──────────┘            └──────────┘             │
│        │                       │                       │                    │
│        │◄──────────────────────┘                       │                    │
│        │      Success                                  │                    │
│        │◄──────────────────────────────────────────────┘                    │
│        │      Success                                                       │
│                                                                             │
│   4. 과반수 복제 확인 → 커밋                                                │
│   ─────────────────────────────                                             │
│                                                                             │
│   Leader: "Index 3이 과반(2/3)에 복제됨 → 커밋!"                           │
│                                                                             │
│   Leader Log:                                                               │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │ Index │ Term │ Command │ Status      │                               │  │
│   ├─────────────────────────────────────────────────────────────────────┤  │
│   │   1   │  1   │ x=1     │ committed   │                               │  │
│   │   2   │  1   │ y=2     │ committed   │                               │  │
│   │   3   │  2   │ x=5     │ committed ★ │ ◄── 커밋됨!                    │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   commitIndex: 2 → 3                                                       │
│                                                                             │
│   5. 클라이언트에 응답 & Followers에 커밋 통지                              │
│   ─────────────────────────────────────────                                 │
│                                                                             │
│   Leader ──────────────────► Client                                        │
│          OK (x=5 committed)                                                │
│                                                                             │
│   다음 AppendEntries에서 leaderCommit=3 전송                               │
│   → Followers도 Index 3까지 커밋                                           │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 4.5. 로그 일관성 보장

Raft는 "Log Matching Property"를 통해 모든 노드의 로그가 일치함을 보장합니다:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      Raft 로그 일관성 메커니즘                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Log Matching Property (로그 매칭 속성)                                    │
│   ─────────────────────────────────────                                     │
│                                                                             │
│   1. 같은 Index와 Term을 가진 엔트리는 같은 Command를 가진다                 │
│   2. 같은 Index와 Term을 가진 엔트리가 있으면, 이전 모든 엔트리도 같다       │
│                                                                             │
│   구현: AppendEntries RPC의 일관성 검사                                     │
│                                                                             │
│   Leader가 AppendEntries 전송 시:                                          │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │ prevLogIndex: 2     ← 새 엔트리 직전의 Index                         │  │
│   │ prevLogTerm: 1      ← 새 엔트리 직전의 Term                          │  │
│   │ entries: [{3,2,x=5}] ← 새 엔트리들                                   │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   Follower 검증:                                                           │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │ "내 Index 2의 Term이 1인가?"                                         │  │
│   │                                                                       │  │
│   │ YES → 일치, 엔트리 추가                                               │  │
│   │ NO  → 불일치, 거부 (Leader가 이전 엔트리부터 재전송)                   │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   불일치 복구 예시:                                                         │
│   ──────────────────                                                        │
│                                                                             │
│   Leader Log:    [1,T1] [2,T1] [3,T2] [4,T2] [5,T3]                        │
│   Follower Log:  [1,T1] [2,T1] [3,T2] [4,T2*]  ← T2*가 다른 Term           │
│                                                                             │
│   1. Leader가 Index 5 전송 (prevLogIndex=4, prevLogTerm=T2)               │
│   2. Follower: "내 Index 4의 Term은 T2*" → 거부                            │
│   3. Leader가 nextIndex를 4로 줄이고 재시도                                │
│   4. Follower: Index 3의 Term은 T2 → 일치                                  │
│   5. Index 4부터 Leader의 로그로 덮어씀                                    │
│                                                                             │
│   결과: Leader의 로그가 항상 승리 (일관성 보장)                              │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 4.6. 쿼럼과 장애 허용

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         쿼럼(Quorum) 계산                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   쿼럼 = 과반수 = ⌊N/2⌋ + 1                                                 │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │ 클러스터 크기 │ 쿼럼 │ 장애 허용 │ 권장           │ 참고             │  │
│   ├─────────────────────────────────────────────────────────────────────┤  │
│   │      1       │  1   │    0     │ 개발만         │ HA 아님          │  │
│   │      2       │  2   │    0     │ 비권장         │ 1노드보다 나쁨   │  │
│   │      3       │  2   │    1     │ ★ 프로덕션 최소│ 가장 일반적      │  │
│   │      4       │  3   │    1     │ 비효율         │ 3노드와 동일     │  │
│   │      5       │  3   │    2     │ ★ 대규모      │ 2노드 장애 허용  │  │
│   │      6       │  4   │    2     │ 비효율         │ 5노드와 동일     │  │
│   │      7       │  4   │    3     │ 드묾          │ 3노드 장애 허용  │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   왜 홀수 노드를 권장하는가?                                                │
│   ─────────────────────────                                                 │
│                                                                             │
│   4노드 클러스터:                                                           │
│   ┌───┐ ┌───┐ ┌───┐ ┌───┐                                                 │
│   │ A │ │ B │ │ C │ │ D │   쿼럼 = 3, 장애 허용 = 1                        │
│   └───┘ └───┘ └───┘ └───┘                                                 │
│                                                                             │
│   3노드 클러스터:                                                           │
│   ┌───┐ ┌───┐ ┌───┐                                                       │
│   │ A │ │ B │ │ C │         쿼럼 = 2, 장애 허용 = 1 (동일!)                │
│   └───┘ └───┘ └───┘                                                       │
│                                                                             │
│   → 4노드는 3노드보다 비용이 33% 더 들지만 가용성 이점 없음                  │
│                                                                             │
│   Split Brain 방지:                                                         │
│   ──────────────────                                                        │
│                                                                             │
│   5노드 클러스터가 네트워크 분할:                                            │
│                                                                             │
│   [A] [B] ─────────X───────── [C] [D] [E]                                  │
│   Partition 1 (2노드)          Partition 2 (3노드)                         │
│   쿼럼 미달 (2 < 3)            쿼럼 충족 (3 ≥ 3)                            │
│   → 쓰기 불가                  → 리더 선출, 쓰기 가능                       │
│                                                                             │
│   → 동시에 두 파티션이 리더를 가지는 것 불가능 (쿼럼이 겹침)                 │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

> **⚠️ 주의 (Warning)**: 2노드 클러스터는 1노드보다 오히려 가용성이 나쁩니다. 쿼럼이 2이므로 1노드만 죽어도 쓰기가 불가능합니다. 차라리 1노드로 운영하거나 3노드로 구성하세요.

### 섹션 요약

- **노드 상태**: Follower, Candidate, Leader
- **Term**: 논리적 시간 단위, 각 Term에 최대 한 명의 리더
- **리더 선출**: 타임아웃 → Candidate → 투표 요청 → 과반 득표 → Leader
- **로그 복제**: 리더가 Follower에 복제 → 과반 복제 시 커밋
- **쿼럼**: 과반수 (N/2 + 1), 홀수 노드 권장

---

## 5. etcd 아키텍처

etcd는 Raft 합의 알고리즘을 사용하는 분산 키-값 저장소입니다. Kubernetes의 모든 클러스터 상태를 저장합니다.

### 5.1. etcd 내부 구조

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          etcd 아키텍처 상세                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   kube-apiserver (클라이언트)                                               │
│        │                                                                    │
│        │ gRPC (TLS, mTLS 인증)                                              │
│        │ 포트: 2379 (client), 2380 (peer)                                  │
│        ▼                                                                    │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │                         etcd Server                                  │  │
│   │                                                                       │  │
│   │  ┌─────────────────────────────────────────────────────────────┐    │  │
│   │  │                     gRPC API Layer                           │    │  │
│   │  │                                                               │    │  │
│   │  │  KV API:                                                     │    │  │
│   │  │  • Put(key, value) - 키-값 저장                              │    │  │
│   │  │  • Get(key) - 값 조회                                        │    │  │
│   │  │  • Delete(key) - 키 삭제                                     │    │  │
│   │  │  • Range(start, end) - 범위 조회                             │    │  │
│   │  │  • Txn(compare, success, failure) - 트랜잭션                 │    │  │
│   │  │                                                               │    │  │
│   │  │  Watch API:                                                  │    │  │
│   │  │  • Watch(key, revision) - 변경 감지                          │    │  │
│   │  │                                                               │    │  │
│   │  │  Lease API:                                                  │    │  │
│   │  │  • Grant(ttl) - 임대 생성                                    │    │  │
│   │  │  • KeepAlive(id) - 임대 갱신                                 │    │  │
│   │  │  • Revoke(id) - 임대 취소                                    │    │  │
│   │  │                                                               │    │  │
│   │  │  Maintenance API:                                            │    │  │
│   │  │  • Snapshot() - 스냅샷 생성                                  │    │  │
│   │  │  • Compact(revision) - 이전 버전 삭제                        │    │  │
│   │  │  • Defragment() - 디스크 조각 모음                           │    │  │
│   │  │                                                               │    │  │
│   │  └─────────────────────────────────────────────────────────────┘    │  │
│   │                              │                                       │  │
│   │                              ▼                                       │  │
│   │  ┌─────────────────────────────────────────────────────────────┐    │  │
│   │  │                     Raft Module                              │    │  │
│   │  │                                                               │    │  │
│   │  │  • Leader Election                                           │    │  │
│   │  │  • Log Replication                                           │    │  │
│   │  │  • Snapshot Transfer                                         │    │  │
│   │  │                                                               │    │  │
│   │  │  구현: etcd/raft 라이브러리                                   │    │  │
│   │  │                                                               │    │  │
│   │  └─────────────────────────────────────────────────────────────┘    │  │
│   │                              │                                       │  │
│   │         ┌────────────────────┴────────────────────┐                 │  │
│   │         ▼                                         ▼                 │  │
│   │  ┌───────────────────┐              ┌───────────────────┐          │  │
│   │  │   WAL (Write-     │              │   BoltDB          │          │  │
│   │  │   Ahead Log)      │              │   (Backend Store) │          │  │
│   │  │                   │              │                   │          │  │
│   │  │ • 순차적 디스크   │              │ • B+ Tree         │          │  │
│   │  │   쓰기 (빠름)     │              │ • MVCC (다중 버전)│          │  │
│   │  │ • 장애 복구용     │              │ • 키-값 저장      │          │  │
│   │  │ • Raft 로그 저장  │              │ • Watch용 히스토리│          │  │
│   │  │                   │              │                   │          │  │
│   │  └───────────────────┘              └───────────────────┘          │  │
│   │         │                                    │                      │  │
│   │         ▼                                    ▼                      │  │
│   │  ┌───────────────────────────────────────────────────────────────┐ │  │
│   │  │                      Disk (SSD 권장)                          │ │  │
│   │  │                                                                 │ │  │
│   │  │  /var/lib/etcd/                                                │ │  │
│   │  │  ├── member/                                                   │ │  │
│   │  │  │   ├── wal/           ← Raft 로그 (WAL)                     │ │  │
│   │  │  │   │   ├── 0000000000000001-0000000000000001.wal            │ │  │
│   │  │  │   │   └── ...                                              │ │  │
│   │  │  │   └── snap/          ← 스냅샷                              │ │  │
│   │  │  │       ├── 0000000000000001-0000000000000001.snap           │ │  │
│   │  │  │       └── db         ← BoltDB (현재 상태)                  │ │  │
│   │  │  └── ...                                                       │ │  │
│   │  └───────────────────────────────────────────────────────────────┘ │  │
│   │                                                                       │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```


### 5.2. etcd의 MVCC (Multi-Version Concurrency Control)

etcd는 키의 모든 버전을 유지하여 일관된 읽기와 Watch를 지원합니다:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           etcd MVCC 구조                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Revision: 전역적으로 증가하는 논리적 시간 (모든 변경마다 증가)             │
│                                                                             │
│   예시: 키 "foo"의 변경 이력                                                │
│                                                                             │
│   시간 ──────────────────────────────────────────────────────────────────►  │
│                                                                             │
│   Revision 1: Put("foo", "bar1")                                           │
│   Revision 2: Put("baz", "qux")    ← 다른 키도 Revision 증가               │
│   Revision 3: Put("foo", "bar2")   ← foo의 두 번째 버전                    │
│   Revision 4: Delete("foo")        ← foo 삭제 (Tombstone)                  │
│   Revision 5: Put("foo", "bar3")   ← foo 재생성                            │
│                                                                             │
│   BoltDB 내부 구조:                                                         │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │   Key Index (B+ Tree)                                               │  │
│   │                                                                       │  │
│   │   Key: "foo"                                                         │  │
│   │   ├── Generation 1: [Revision 1 → Revision 4 (Tombstone)]           │  │
│   │   └── Generation 2: [Revision 5 → ...]                              │  │
│   │                                                                       │  │
│   │   Key: "baz"                                                         │  │
│   │   └── Generation 1: [Revision 2 → ...]                              │  │
│   │                                                                       │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │   Backend (Revision → Value)                                        │  │
│   │                                                                       │  │
│   │   Revision 1: ("foo", "bar1", CreateRev=1, ModRev=1, Version=1)     │  │
│   │   Revision 2: ("baz", "qux", CreateRev=2, ModRev=2, Version=1)      │  │
│   │   Revision 3: ("foo", "bar2", CreateRev=1, ModRev=3, Version=2)     │  │
│   │   Revision 4: ("foo", Tombstone, CreateRev=1, ModRev=4, Version=3)  │  │
│   │   Revision 5: ("foo", "bar3", CreateRev=5, ModRev=5, Version=1)     │  │
│   │                                                                       │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   Watch 동작:                                                               │
│   ───────────                                                               │
│   • Watch("foo", start_revision=3)                                         │
│   • Revision 3 이후의 모든 "foo" 변경을 스트리밍                            │
│   • Kubernetes Controller가 이 메커니즘으로 리소스 변경을 감지              │
│                                                                             │
│   Compaction:                                                               │
│   ────────────                                                              │
│   • 오래된 Revision 삭제 (디스크 공간 확보)                                 │
│   • Compact(revision=4) → Revision 4 이전 데이터 삭제                      │
│   • 주의: Compaction 전 Revision에 대한 Watch/읽기 불가                     │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 5.3. Kubernetes가 etcd를 사용하는 방식

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                   Kubernetes ↔ etcd 상호작용                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   키 구조: /registry/{resource-type}/{namespace}/{name}                     │
│                                                                             │
│   예시:                                                                     │
│   /registry/pods/default/nginx-deployment-5d8c4b7f9-abc12                  │
│   /registry/services/kube-system/kube-dns                                  │
│   /registry/configmaps/kube-system/coredns                                 │
│   /registry/secrets/default/my-secret                                      │
│   /registry/leases/kube-system/kube-scheduler                              │
│                                                                             │
│   값 형식: Protobuf 직렬화된 Kubernetes 오브젝트                            │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │  Kubernetes 오브젝트                                                 │  │
│   │                                                                       │  │
│   │  apiVersion: v1                                                      │  │
│   │  kind: Pod                                                           │  │
│   │  metadata:                                                           │  │
│   │    name: nginx                                                       │  │
│   │    namespace: default                                                │  │
│   │    uid: abc-123-...                                                  │  │
│   │    resourceVersion: "12345"  ◄── etcd Revision과 매핑               │  │
│   │  spec:                                                               │  │
│   │    containers: [...]                                                 │  │
│   │  status:                                                             │  │
│   │    phase: Running                                                    │  │
│   │                                                                       │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   resourceVersion의 역할:                                                   │
│   ─────────────────────────                                                 │
│                                                                             │
│   1. Watch 시작점                                                          │
│      kubectl get pods --watch --resource-version=12345                    │
│      → Revision 12345 이후의 변경만 수신                                    │
│                                                                             │
│   2. 낙관적 동시성 제어                                                     │
│      Client A: Get Pod (resourceVersion=100)                               │
│      Client A: Update Pod (resourceVersion=100)                            │
│      API Server: "OK" (resourceVersion → 101)                              │
│                                                                             │
│      Client B: Get Pod (resourceVersion=100) ← 이전에 가져옴               │
│      Client B: Update Pod (resourceVersion=100)                            │
│      API Server: "Conflict!" (현재 버전은 101)                             │
│                                                                             │
│   3. List/Watch 일관성                                                     │
│      List 결과와 함께 반환된 resourceVersion부터 Watch 시작                 │
│      → 누락이나 중복 없이 모든 변경 수신                                    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 5.4. etcd 클러스터 관리 명령어

```bash
# 환경 변수 설정 (편의상)
export ETCDCTL_API=3
export ETCDCTL_ENDPOINTS=https://127.0.0.1:2379
export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt
export ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt
export ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key

# 클러스터 상태 확인
etcdctl endpoint status --write-out=table
# +------------------------+------------------+---------+---------+-----------+
# |        ENDPOINT        |        ID        | VERSION | DB SIZE | IS LEADER |
# +------------------------+------------------+---------+---------+-----------+
# | https://192.168.108.11 | 8e9e05c52164694d | 3.5.12  |  4.5 MB |     true  |
# | https://192.168.108.12 | 91bc3c398fb3c146 | 3.5.12  |  4.5 MB |    false  |
# | https://192.168.108.13 | fd422379fda50e48 | 3.5.12  |  4.5 MB |    false  |
# +------------------------+------------------+---------+---------+-----------+

# 멤버 목록
etcdctl member list --write-out=table

# 클러스터 헬스 체크
etcdctl endpoint health --write-out=table

# 리더 확인
etcdctl endpoint status --write-out=json | jq -r '.[] | select(.Status.leader==.Status.header.member_id) | .Endpoint'

# 키 조회 (주의: 프로덕션에서 조심)
etcdctl get /registry/pods/default --prefix --keys-only | head -20

# 키 개수 확인
etcdctl get / --prefix --keys-only | wc -l

# 스냅샷 백업 (필수!)
etcdctl snapshot save /backup/etcd-$(date +%Y%m%d-%H%M%S).db

# 스냅샷 상태 확인
etcdctl snapshot status /backup/etcd-20240101-120000.db --write-out=table
# +----------+----------+------------+------------+
# |   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
# +----------+----------+------------+------------+
# | 3c3b8c6f |   123456 |       8542 |     4.5 MB |
# +----------+----------+------------+------------+

# Compaction (오래된 리비전 정리)
# 현재 리비전 확인
CURRENT_REV=$(etcdctl endpoint status --write-out=json | jq -r '.[0].Status.header.revision')
# 1시간 전 리비전까지 유지하고 이전 삭제 (예시)
etcdctl compact $((CURRENT_REV - 10000))

# Defragmentation (디스크 조각 모음)
etcdctl defrag --endpoints=https://192.168.108.11:2379
# 주의: 각 노드에 개별 실행, 리더는 마지막에

# 알람 확인/해제 (공간 부족 등)
etcdctl alarm list
etcdctl alarm disarm
```

> **💡 팁 (Tip)**: etcd 백업은 **정기적으로, 자동으로** 수행하세요. etcd 데이터 손실 = Kubernetes 클러스터 전체 상태 손실입니다. CronJob으로 시간당/일당 백업을 구성하고, 외부 스토리지에 보관하세요.

### 섹션 요약

- **etcd 구조**: gRPC API → Raft Module → WAL/BoltDB
- **MVCC**: 모든 키의 버전 이력 유지, Watch 지원
- **Kubernetes 연동**: resourceVersion으로 일관성 보장
- **운영**: 정기 백업, Compaction, Defragmentation 필수

---

## 6. 장애 시나리오와 복구

### 6.1. 일반적인 장애 시나리오

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                       etcd 장애 시나리오                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   시나리오 1: 단일 노드 장애 (쿼럼 유지)                                     │
│   ─────────────────────────────────────                                     │
│                                                                             │
│   [Node A] [Node B] [Node C]        →        [Node A] [Node B]  X           │
│    Leader  Follower Follower                   Leader  Follower (Down)      │
│                                                                             │
│   영향: 없음 (쿼럼 2/3 유지)                                                │
│   조치: 장애 노드 복구 또는 새 노드 추가                                    │
│   복구:                                                                     │
│   1. 노드 복구 시 자동으로 클러스터에 재참여                                 │
│   2. 리더에서 로그 동기화 후 정상 동작                                       │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   시나리오 2: 리더 노드 장애                                                │
│   ─────────────────────────────                                             │
│                                                                             │
│   [Node A] [Node B] [Node C]        →        X [Node B] [Node C]            │
│    Leader  Follower Follower              (Down) Candidate                  │
│     (X)                                           ↓ 선거                     │
│                                              [Node B] [Node C]              │
│                                               Leader  Follower              │
│                                                                             │
│   영향: 선거 완료까지 쓰기 불가 (수 초)                                      │
│   자동 복구: 새 리더 선출 (150-300ms 타임아웃 후)                           │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   시나리오 3: 쿼럼 상실 (2노드 장애)                                        │
│   ───────────────────────────────                                           │
│                                                                             │
│   [Node A] [Node B] [Node C]        →        X X [Node C]                   │
│                                          (Down) (Down) (쿼럼 없음)          │
│                                                                             │
│   영향: 클러스터 전체 쓰기 불가 (읽기만 가능)                                │
│   조치:                                                                     │
│   - 최소 1노드 복구하여 쿼럼 회복 (권장)                                     │
│   - 또는: 단일 노드로 강제 복구 (데이터 손실 위험)                           │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   시나리오 4: 네트워크 분할 (Split Brain 방지)                               │
│   ─────────────────────────────────────────────                             │
│                                                                             │
│   [Node A] [Node B] ─────X───── [Node C]                                   │
│    Leader  Follower              Follower                                   │
│      ↓ 네트워크 분할                                                        │
│                                                                             │
│   Partition 1: [A] [B] (쿼럼 충족: 2/3)                                    │
│   Partition 2: [C] (쿼럼 미달: 1/3)                                        │
│                                                                             │
│   Partition 1:                                                              │
│   - A는 리더 유지 (heartbeat가 B에게 도달)                                  │
│   - 쓰기/읽기 모두 정상                                                     │
│                                                                             │
│   Partition 2:                                                              │
│   - C는 선거 시도하지만 쿼럼 미달로 리더 불가                                │
│   - 읽기만 가능 (stale read 가능성)                                         │
│   - 쓰기 불가                                                               │
│                                                                             │
│   → 네트워크 복구 시 C가 A의 리더십 인정, 로그 동기화                        │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 6.2. 재해 복구 절차

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                       etcd 재해 복구 가이드                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   복구 방법 1: 스냅샷에서 복구 (권장)                                        │
│   ─────────────────────────────────                                         │
│                                                                             │
│   전제: 정기 백업된 스냅샷이 있음                                            │
│                                                                             │
│   1. 기존 etcd 데이터 백업                                                  │
│   ```bash                                                                   │
│   sudo mv /var/lib/etcd /var/lib/etcd.backup                              │
│   ```                                                                       │
│                                                                             │
│   2. 스냅샷에서 복구 (모든 Control Plane 노드에서)                          │
│   ```bash                                                                   │
│   # Node 1                                                                  │
│   sudo etcdctl snapshot restore /backup/etcd-snapshot.db \                 │
│     --name=etcd-1 \                                                        │
│     --data-dir=/var/lib/etcd \                                             │
│     --initial-cluster=etcd-1=https://192.168.108.11:2380,\                 │
│                        etcd-2=https://192.168.108.12:2380,\                │
│                        etcd-3=https://192.168.108.13:2380 \                │
│     --initial-cluster-token=etcd-cluster-restored \                        │
│     --initial-advertise-peer-urls=https://192.168.108.11:2380              │
│   ```                                                                       │
│                                                                             │
│   3. etcd 서비스 재시작 (모든 노드)                                         │
│   ```bash                                                                   │
│   sudo systemctl restart etcd                                              │
│   # 또는 kubelet이 관리하는 경우:                                           │
│   sudo systemctl restart kubelet                                           │
│   ```                                                                       │
│                                                                             │
│   4. 복구 확인                                                              │
│   ```bash                                                                   │
│   etcdctl endpoint status --write-out=table                                │
│   kubectl get nodes                                                        │
│   ```                                                                       │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   복구 방법 2: 단일 노드에서 강제 클러스터 재구성                             │
│   ─────────────────────────────────────────────                             │
│                                                                             │
│   경고: 마지막 수단, 데이터 손실 가능                                        │
│                                                                             │
│   상황: 3노드 중 2노드가 완전히 손실, 1노드만 남음                           │
│                                                                             │
│   1. 남은 노드에서 etcd 중지                                                │
│   ```bash                                                                   │
│   sudo systemctl stop etcd                                                 │
│   ```                                                                       │
│                                                                             │
│   2. etcd를 단일 노드 모드로 강제 재구성                                     │
│   ```bash                                                                   │
│   sudo etcdctl snapshot save /tmp/snapshot.db                              │
│   sudo mv /var/lib/etcd /var/lib/etcd.old                                 │
│   sudo etcdctl snapshot restore /tmp/snapshot.db \                         │
│     --name=etcd-1 \                                                        │
│     --data-dir=/var/lib/etcd \                                             │
│     --initial-cluster=etcd-1=https://192.168.108.11:2380 \                 │
│     --initial-cluster-token=etcd-cluster-single \                          │
│     --initial-advertise-peer-urls=https://192.168.108.11:2380              │
│   ```                                                                       │
│                                                                             │
│   3. etcd 구성 파일 수정 (단일 노드용)                                       │
│                                                                             │
│   4. etcd 시작 및 새 노드 추가                                              │
│   ```bash                                                                   │
│   sudo systemctl start etcd                                                │
│   # 새 노드 추가                                                            │
│   etcdctl member add etcd-2 --peer-urls=https://192.168.108.12:2380       │
│   etcdctl member add etcd-3 --peer-urls=https://192.168.108.13:2380       │
│   ```                                                                       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

> **⚠️ 주의 (Warning)**: 프로덕션 환경에서는 **반드시 정기 백업**을 설정하세요. etcd 데이터 손실은 Kubernetes 클러스터 전체 상태 손실을 의미합니다. 최소 시간당 1회 백업, 백업 파일은 외부 스토리지에 보관하세요.

### 섹션 요약

- **단일 노드 장애**: 자동 복구, 영향 없음
- **리더 장애**: 새 리더 선출 (수 초 내)
- **쿼럼 상실**: 쓰기 불가, 노드 복구 또는 강제 재구성 필요
- **네트워크 분할**: 쿼럼 있는 파티션만 정상 동작
- **복구**: 스냅샷 기반 복구 권장

---

## 이 장의 요약

| 개념 | 설명 | Kubernetes에서의 역할 |
|------|------|---------------------|
| **분산 시스템** | 여러 노드가 네트워크로 연결되어 하나처럼 동작 | 클러스터 전체 구조 |
| **CAP 정리** | C, A, P 중 2가지만 선택 가능 | etcd는 CP (일관성 우선) |
| **합의 문제** | 여러 노드가 같은 값에 동의 | 클러스터 상태 일관성 |
| **Raft** | 이해하기 쉬운 합의 알고리즘 | etcd의 핵심 엔진 |
| **리더 선출** | 타임아웃 → 후보 → 투표 → 리더 | etcd 클러스터 조율 |
| **로그 복제** | 리더가 Follower에 복제 | 데이터 일관성 보장 |
| **쿼럼** | 과반수 (N/2 + 1) | 장애 허용, Split Brain 방지 |
| **MVCC** | 다중 버전 동시성 제어 | Watch, resourceVersion |
| **스냅샷/백업** | 현재 상태의 전체 복사본 | 재해 복구 |

---

## 실습 과제

### 과제 1: etcd 클러스터 상태 모니터링

**목표**: etcd 클러스터의 상태를 확인하고 모니터링하는 방법을 익힙니다.

```bash
# 환경 변수 설정
export ETCDCTL_API=3
export ETCDCTL_ENDPOINTS=https://127.0.0.1:2379
export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt
export ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt
export ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key

# 1. 클러스터 엔드포인트 상태
etcdctl endpoint status --write-out=table

# 2. 멤버 목록 확인
etcdctl member list --write-out=table

# 3. 헬스 체크
etcdctl endpoint health --write-out=table

# 4. 리더 식별
etcdctl endpoint status --write-out=json | jq '.[] | {endpoint: .Endpoint, isLeader: (.Status.leader == .Status.header.member_id)}'

# 5. DB 크기 확인
etcdctl endpoint status --write-out=json | jq '.[] | {endpoint: .Endpoint, dbSize: .Status.dbSize, dbSizeInUse: .Status.dbSizeInUse}'

# 6. 알람 확인
etcdctl alarm list
```

**도전 과제**:
- Prometheus/Grafana로 etcd 메트릭을 시각화해보세요
- etcd 메트릭 엔드포인트(/metrics)를 확인하세요

---

### 과제 2: etcd 백업 및 복구 테스트

**목표**: etcd 스냅샷을 생성하고 복구하는 절차를 연습합니다.

```bash
# 1. 현재 데이터 확인
kubectl get pods -A | wc -l
kubectl get configmaps -A | wc -l

# 2. 스냅샷 백업
sudo etcdctl snapshot save /tmp/etcd-backup-test.db

# 3. 스냅샷 검증
etcdctl snapshot status /tmp/etcd-backup-test.db --write-out=table

# 4. 테스트: 새 리소스 생성
kubectl create configmap test-backup-cm --from-literal=key=value

# 5. 확인
kubectl get configmap test-backup-cm

# 6. (주의: 실제 복구 테스트는 비프로덕션 환경에서!)
# 복구 절차는 위의 "재해 복구 절차" 참조

# 7. 테스트 정리
kubectl delete configmap test-backup-cm
```

**도전 과제**:
- CronJob으로 자동 백업 스크립트를 작성해보세요
- 외부 스토리지(S3, NFS)로 백업 파일을 복사하는 스크립트를 추가하세요

---

### 과제 3: 리더 선출 관찰

**목표**: etcd 리더 선출 과정을 실시간으로 관찰합니다.

```bash
# 터미널 1: 리더 모니터링 (1초마다)
watch -n 1 'etcdctl endpoint status --write-out=table 2>/dev/null'

# 터미널 2: etcd 로그 모니터링 (Control Plane 노드에서)
sudo journalctl -u etcd -f
# 또는 kubelet으로 실행되는 경우:
kubectl logs -n kube-system -l component=etcd -f

# 터미널 3: (주의: 비프로덕션에서만!) 리더 노드의 etcd 재시작
# 리더 노드를 확인하고 해당 노드에서:
# sudo systemctl restart etcd

# 관찰 포인트:
# - 리더가 변경되는 시점
# - 새 리더 선출까지 걸리는 시간
# - Follower 로그의 "elected leader" 메시지
```

**도전 과제**:
- 네트워크 지연을 시뮬레이션하여 리더 선출을 관찰하세요 (`tc netem`)
- 선거 타임아웃 설정이 리더 선출에 미치는 영향을 확인하세요

---

### 과제 4: Kubernetes 리소스의 etcd 저장 확인

**목표**: Kubernetes 리소스가 etcd에 어떻게 저장되는지 확인합니다.

```bash
# 1. 테스트 리소스 생성
kubectl create namespace etcd-test
kubectl create configmap test-cm -n etcd-test --from-literal=hello=world

# 2. etcd에서 직접 조회 (주의: 프로덕션에서 조심!)
etcdctl get /registry/configmaps/etcd-test/test-cm --print-value-only | head -c 200
# (Protobuf 형식이라 바이너리로 보임)

# 3. 키 존재 확인
etcdctl get /registry/configmaps/etcd-test/test-cm --keys-only

# 4. Namespace의 모든 ConfigMap 키 조회
etcdctl get /registry/configmaps/etcd-test --prefix --keys-only

# 5. 리소스 삭제 후 etcd 확인
kubectl delete configmap test-cm -n etcd-test
etcdctl get /registry/configmaps/etcd-test/test-cm --keys-only
# (키가 없어짐, 또는 Tombstone)

# 6. 정리
kubectl delete namespace etcd-test
```

**도전 과제**:
- Secret이 etcd에 어떻게 저장되는지 확인하세요 (암호화 여부)
- `etcd-encryption` 설정을 적용하고 Secret 암호화를 확인하세요

---

### 과제 5: 쿼럼 동작 확인 (비프로덕션 전용)

**목표**: 쿼럼이 클러스터 가용성에 미치는 영향을 확인합니다.

```bash
# 경고: 비프로덕션 환경에서만 수행하세요!

# 1. 3노드 클러스터 상태 확인
etcdctl member list --write-out=table
etcdctl endpoint status --write-out=table

# 2. 1노드 중지 (쿼럼 유지: 2/3)
# 특정 노드에서:
# sudo systemctl stop etcd

# 3. 쓰기 테스트 (성공해야 함)
kubectl create configmap quorum-test --from-literal=test=1

# 4. 2노드 중지 (쿼럼 상실: 1/3)
# 추가 노드에서:
# sudo systemctl stop etcd

# 5. 쓰기 테스트 (실패해야 함)
kubectl create configmap quorum-test-2 --from-literal=test=2
# 결과: 타임아웃 또는 에러

# 6. 읽기 테스트 (성공할 수 있음 - stale read)
kubectl get configmap quorum-test

# 7. 노드 복구
# 중지했던 노드들에서:
# sudo systemctl start etcd

# 8. 정상 동작 확인
kubectl create configmap quorum-test-3 --from-literal=test=3
kubectl delete configmap quorum-test quorum-test-3
```

---

## 학습 점검

### 질문 1: CAP 정리에서 Kubernetes/etcd가 CP를 선택한 이유는?

<details>
<summary>정답 보기</summary>

**일관성(Consistency)이 가용성(Availability)보다 중요하기 때문입니다.**

Kubernetes 클러스터에서 데이터 불일치가 발생하면:
- Pod가 중복 생성될 수 있음
- 이미 삭제된 Service가 다시 나타날 수 있음
- 설정이 의도치 않게 롤백될 수 있음
- Controller들이 충돌하는 작업을 수행할 수 있음

반면 일시적인 불가용(쿼럼 상실 시 쓰기 거부)은:
- 기존 워크로드는 계속 실행됨
- 새 변경만 일시적으로 불가
- 쿼럼 복구 시 정상화

"잘못된 데이터"보다 "일시적으로 변경 불가"가 훨씬 안전합니다.

</details>

---

### 질문 2: Raft에서 선거 타임아웃이 랜덤인 이유는?

<details>
<summary>정답 보기</summary>

**Split Vote(투표 분산)를 방지하기 위해서입니다.**

모든 노드가 동일한 타임아웃을 가지면:
1. 리더 장애 시 모든 Follower가 동시에 타임아웃
2. 모든 Follower가 동시에 Candidate로 전환
3. 각자 자신에게 투표 → 투표 분산 → 아무도 과반 미달성
4. 다시 타임아웃 → 반복 (선거 지연)

랜덤 타임아웃 (150-300ms):
1. 한 노드가 먼저 타임아웃되어 Candidate로 전환
2. 다른 노드는 아직 Follower 상태
3. 먼저 타임아웃된 노드가 투표 요청 → 대부분 득표
4. 빠르게 리더 선출

</details>

---

### 질문 3: 5노드 etcd 클러스터에서 몇 개 노드가 장애나면 쓰기가 불가능한가?

<details>
<summary>정답 보기</summary>

**3개 이상 장애 시 쓰기 불가**

계산:
- 5노드 클러스터의 쿼럼 = ⌊5/2⌋ + 1 = 3
- 쓰기를 위해 최소 3개 노드가 동의해야 함
- 따라서 최대 2개 노드 장애까지 허용

장애 시나리오:
- 0-1개 장애: 정상 동작 (쿼럼 충족)
- 2개 장애: 3/5 노드 정상 → 쓰기 가능
- 3개 장애: 2/5 노드 정상 → 쿼럼 미달 → 쓰기 불가

</details>

---

### 질문 4: etcd의 MVCC가 Kubernetes Watch에 어떻게 활용되는가?

<details>
<summary>정답 보기</summary>

**MVCC는 모든 키의 버전 이력을 유지하여 Watch가 특정 시점부터 변경을 수신할 수 있게 합니다.**

동작:
1. 클라이언트가 List 요청 → 현재 데이터 + resourceVersion 반환
2. Watch 요청 시 해당 resourceVersion부터 시작
3. etcd는 그 Revision 이후의 모든 변경을 스트리밍
4. 놓친 이벤트 없이 모든 변경 수신

Kubernetes Controller 예시:
```
1. Controller 시작
2. List Pods → resourceVersion: 12345
3. Watch(resourceVersion=12345) 시작
4. Revision 12346: Pod 생성 → Watch 이벤트 수신
5. Revision 12347: Pod 삭제 → Watch 이벤트 수신
...
```

연결이 끊어져도 마지막 resourceVersion부터 재연결하면 이벤트 누락 없음.

</details>

---

### 질문 5: 네트워크 분할 시 왜 한쪽 파티션만 리더를 가질 수 있는가?

<details>
<summary>정답 보기</summary>

**쿼럼(과반수) 요구 사항 때문입니다.**

5노드 클러스터가 [A,B] | [C,D,E]로 분할된 경우:

Partition 1 (A,B - 2노드):
- 쿼럼(3) 미달 → 리더 선출 불가
- 기존 리더가 있어도 heartbeat가 쿼럼에 도달 못함 → 리더십 상실

Partition 2 (C,D,E - 3노드):
- 쿼럼(3) 충족 → 리더 선출 가능
- 새 리더가 선출되어 정상 동작

핵심: 어떤 분할에서도 **두 파티션이 동시에 쿼럼을 충족할 수 없음**
- N노드 클러스터의 쿼럼 = N/2 + 1
- 두 파티션의 합 = N
- 두 파티션 모두 쿼럼 충족 불가능 (수학적으로 증명됨)

→ Split Brain (두 개의 리더) 방지

</details>

---

### 질문 6: etcd 백업을 "정기적으로" 해야 하는 이유는?

<details>
<summary>정답 보기</summary>

**etcd 데이터 손실 = Kubernetes 클러스터 전체 상태 손실이기 때문입니다.**

etcd에 저장되는 것:
- 모든 Pod, Deployment, Service, ConfigMap, Secret 정의
- 클러스터 설정
- RBAC 정책
- Custom Resources

백업이 없으면:
- 클러스터 재구성 필요 (수작업)
- 모든 애플리케이션 재배포 필요
- 설정, 시크릿 등 손실

권장 사항:
- 최소 시간당 1회 자동 백업
- 백업 파일은 etcd 노드 외부에 저장 (S3, NFS 등)
- 복구 절차를 정기적으로 테스트
- 여러 버전의 백업 보관 (최근 24시간, 최근 7일 등)

</details>

---

### 질문 7: Raft 로그 복제에서 "커밋"이 의미하는 것은?

<details>
<summary>정답 보기</summary>

**커밋 = 로그 엔트리가 과반수 노드에 복제되어 손실되지 않음이 보장된 상태**

커밋 과정:
1. 리더가 새 로그 엔트리 추가 (uncommitted)
2. 리더가 Follower들에게 AppendEntries 전송
3. Follower들이 로그에 추가하고 응답
4. 리더가 과반수 응답 확인 → **커밋**
5. 리더가 클라이언트에 성공 응답
6. 다음 AppendEntries에서 Follower에게 커밋 알림

커밋의 의미:
- 해당 로그는 어떤 노드가 죽어도 손실되지 않음
- 새 리더가 선출되어도 해당 로그는 반드시 포함
- 클라이언트에게 "성공"으로 응답 가능

커밋 전:
- 일부 노드에만 존재할 수 있음
- 새 리더 선출 시 덮어쓰일 수 있음
- 클라이언트에게 아직 응답하지 않음

</details>

---

### 질문 8: etcd Compaction의 목적과 주의사항은?

<details>
<summary>정답 보기</summary>

**목적: 오래된 키 버전을 삭제하여 디스크 공간 확보**

etcd MVCC는 모든 키의 모든 버전을 유지합니다. 시간이 지나면:
- 디스크 사용량 증가
- 쿼리 성능 저하
- 백업 크기 증가

Compaction:
```bash
# Revision 12345 이전의 모든 이전 버전 삭제
etcdctl compact 12345
```

주의사항:
1. **Watch 깨짐**: Compacted revision 이전으로 Watch 재연결 불가
   - 에러: "required revision has been compacted"
   - Controller가 재시작 시 List부터 다시 시작해야 함

2. **복구 불가**: Compaction 후 이전 버전 조회 불가

3. **Defragmentation 필요**: Compaction은 데이터를 삭제 표시만 함
   - 실제 디스크 공간 회수는 `etcdctl defrag` 필요

권장:
- Kubernetes에서는 kube-apiserver가 자동으로 Compaction 수행
- `--etcd-compaction-interval` 플래그로 주기 설정

</details>

---

### 질문 9: 홀수 노드가 짝수 노드보다 권장되는 이유는?

<details>
<summary>정답 보기</summary>

**짝수 노드는 홀수 노드와 같은 장애 허용 능력을 가지면서 비용만 더 들기 때문입니다.**

비교:
| 노드 수 | 쿼럼 | 장애 허용 |
|---------|------|----------|
| 3       | 2    | 1        |
| 4       | 3    | 1 (동일!) |
| 5       | 3    | 2        |
| 6       | 4    | 2 (동일!) |

4노드 vs 3노드:
- 4노드: 2노드 장애 시 쿼럼 미달 (2/4 < 3)
- 3노드: 2노드 장애 시 쿼럼 미달 (1/3 < 2)
- 결국 둘 다 1노드 장애까지만 허용

4노드의 추가적인 문제:
- Split Brain 위험 증가: 2|2 분할 시 양쪽 모두 쿼럼 미달
- 3노드: 1|2 분할 시 2노드 쪽이 쿼럼 충족

비용 대비 효과:
- 4노드는 3노드보다 33% 비용 증가
- 가용성 이점 없음

</details>

---

### 질문 10: Raft에서 새 리더가 반드시 모든 커밋된 로그를 가지는 이유는?

<details>
<summary>정답 보기</summary>

**투표 규칙 때문입니다: Follower는 자신보다 로그가 최신인 후보에게만 투표합니다.**

Raft 안전성 보장:
1. 커밋 = 과반수에 복제됨
2. 새 리더 선출 = 과반수의 투표 필요
3. 과반수 두 집합은 반드시 겹침 (비둘기집 원리)
4. 겹치는 노드는 커밋된 로그를 가짐
5. 그 노드는 더 오래된 로그를 가진 후보에게 투표 안 함
6. 따라서 새 리더는 모든 커밋된 로그를 가짐

예시:
```
5노드 클러스터, 로그 Index 10까지 커밋됨 (3노드에 복제)

노드: [A:Index 10] [B:Index 10] [C:Index 10] [D:Index 9] [E:Index 9]

D가 후보가 되어 투표 요청:
- A: "내 로그가 더 최신" → 거부
- B: "내 로그가 더 최신" → 거부  
- C: "내 로그가 더 최신" → 거부
- E: 동의

D는 2표 (자신 + E)로 과반 미달 → 당선 불가

A가 후보가 되면:
- B,C,D,E 모두 투표 가능 → 당선

→ 새 리더(A)는 Index 10까지 모든 커밋 로그 보유
```

</details>

---

## 다음 단계

이 장에서 분산 시스템의 이론과 etcd를 학습했습니다. 다음 장에서는 **eBPF와 Cilium**을 심층적으로 다룹니다:

- **eBPF 아키텍처**: 커널 내 프로그래밍의 혁신
- **Cilium의 동작 원리**: eBPF 기반 CNI
- **kube-proxy 대체**: eBPF로 Service 로드밸런싱
- **Hubble 관찰성**: 네트워크 흐름 시각화

Cilium은 기존 iptables 기반 네트워킹의 한계를 극복하고, 커널 수준에서 네트워킹, 보안, 관찰성을 제공합니다. etcd가 클러스터의 "두뇌"라면, Cilium은 클러스터의 "신경계"입니다.

---

## 참고 자료

### 공식 문서

- [etcd Documentation](https://etcd.io/docs/)
- [Raft Consensus Algorithm](https://raft.github.io/)
- [Kubernetes etcd Configuration](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/)

### 논문

- [In Search of an Understandable Consensus Algorithm (Raft)](https://raft.github.io/raft.pdf)
- [Paxos Made Simple](https://lamport.azurewebsites.net/pubs/paxos-simple.pdf)
- [CAP Twelve Years Later](https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed/)

### 심화 학습

- [Designing Data-Intensive Applications (O'Reilly)](https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/) - Chapter 5, 9
- [The Raft Visualization](https://raft.github.io/) - 인터랙티브 시각화
- [etcd Source Code](https://github.com/etcd-io/etcd)

### 도구

- `etcdctl` - etcd CLI 도구
- `etcd-dump-logs` - WAL 로그 분석
- `etcd-io/bbolt` - BoltDB CLI

---

> **💡 팁 (Tip)**: 분산 시스템을 진정으로 이해하려면 장애 상황을 직접 시뮬레이션해보세요. 테스트 환경에서 노드를 죽이고, 네트워크를 끊고, 클러스터가 어떻게 반응하는지 관찰하세요. 이론만으로는 알 수 없는 통찰을 얻을 수 있습니다.
