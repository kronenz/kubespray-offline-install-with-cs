# eBPF와 Cilium 심화

> *"eBPF는 Linux 커널의 JavaScript와 같습니다. 
> 커널을 다시 컴파일하거나 재부팅하지 않고도 커널의 동작을 프로그래밍할 수 있습니다."*
> — Brendan Gregg (Netflix 성능 엔지니어, BPF Performance Tools 저자)

2014년까지 Linux 커널을 확장하려면 커널 모듈을 작성해야 했습니다. 이는 위험하고(커널 패닉 가능), 느리며(재부팅 필요), 이식성이 없었습니다(커널 버전마다 다른 코드). eBPF(extended Berkeley Packet Filter)는 이 모든 것을 바꿨습니다.

eBPF를 사용하면 커널 내부에서 안전하게 실행되는 프로그램을 작성할 수 있습니다. Verifier가 프로그램의 안전성을 보장하고, JIT 컴파일러가 네이티브 성능을 제공합니다. 네트워킹, 보안, 관찰성 분야에서 혁명적인 변화를 가져왔으며, Cilium, Falco, Datadog 등 현대적인 인프라 도구의 핵심 기술입니다.

Cilium은 eBPF를 활용하여 Kubernetes 네트워킹을 완전히 새롭게 구현했습니다. 기존 iptables 기반의 kube-proxy는 Service가 증가할수록 O(n) 성능 저하를 보였지만, Cilium의 eBPF 기반 구현은 O(1) 해시 테이블 룩업으로 대규모 클러스터에서도 일관된 성능을 제공합니다.

이 장에서는 eBPF의 내부 동작 원리와 Cilium이 이를 어떻게 활용하여 고성능 네트워킹, 보안 정책, 관찰성을 제공하는지 심층적으로 탐구합니다.

---

## 이 장에서 다루는 내용

이 장을 읽고 나면 다음을 이해할 수 있습니다:

- **eBPF 아키텍처**: Verifier, JIT, Maps, Helper Functions
- **eBPF Hook Points**: XDP, TC, Socket, Cgroup 훅의 차이와 용도
- **Cilium 컴포넌트**: Agent, Operator, eBPF Datapath
- **kube-proxy 대체**: eBPF 기반 Service 로드밸런싱의 원리
- **Datapath 모드**: veth vs netkit (Linux 6.7+)
- **Maglev와 DSR**: 고급 로드밸런싱 기법
- **네트워크 정책**: L3/L4/L7 정책의 eBPF 구현
- **Hubble 관찰성**: eBPF 기반 네트워크 흐름 시각화

> **📘 개념 (Concept)**: eBPF를 이해하면 Cilium의 동작 원리뿐만 아니라 현대 Linux 시스템 전반의 관찰성과 보안 도구를 이해할 수 있습니다. Falco(보안), Datadog(모니터링), Pixie(관찰성) 등 모두 eBPF를 기반으로 합니다.

---

## 1. eBPF 개요

### 1.1. 전통적 커널 확장의 문제

Linux 커널 기능을 확장하는 전통적인 방법은 커널 모듈(Loadable Kernel Module, LKM)이었습니다:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     커널 모듈 vs eBPF 비교                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   커널 모듈 (전통적 방식)                                                   │
│   ─────────────────────────                                                 │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │  개발자 ──► C 코드 ──► 컴파일 ──► .ko 파일 ──► insmod ──► 커널     │  │
│   │                                                                       │  │
│   │  문제점:                                                              │  │
│   │  • 커널 버전마다 다시 컴파일 필요                                     │  │
│   │  • 버그가 있으면 커널 패닉 (시스템 전체 다운)                         │  │
│   │  • 보안 취약점이 될 수 있음                                           │  │
│   │  • 프로덕션 환경에서 테스트 어려움                                    │  │
│   │  • 로드/언로드 시 재부팅 필요할 수 있음                                │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   eBPF (현대적 방식)                                                        │
│   ─────────────────────                                                     │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │                                                                       │  │
│   │  개발자 ──► C 코드 ──► eBPF 바이트코드 ──► Verifier ──► JIT ──► 실행 │  │
│   │                                              │                        │  │
│   │                                              │ 안전성 검증            │  │
│   │                                              ▼                        │  │
│   │                                        ┌──────────┐                  │  │
│   │                                        │ 통과/거부 │                  │  │
│   │                                        └──────────┘                  │  │
│   │                                                                       │  │
│   │  장점:                                                                │  │
│   │  • CO-RE (Compile Once, Run Everywhere): 커널 버전 독립적            │  │
│   │  • Verifier가 안전성 보장: 커널 패닉 불가능                           │  │
│   │  • 동적 로드/언로드: 재부팅 없이 즉시 적용                            │  │
│   │  • 제한된 권한: CAP_BPF로 세밀한 권한 제어                            │  │
│   │  • 프로덕션에서 안전하게 실행                                         │  │
│   │                                                                       │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   ┌───────────────────┬──────────────────┬─────────────────────────────┐  │
│   │     특성          │  커널 모듈       │  eBPF                       │  │
│   ├───────────────────┼──────────────────┼─────────────────────────────┤  │
│   │  안전성          │  커널 패닉 가능   │  Verifier가 보장            │  │
│   │  이식성          │  커널 버전 의존   │  CO-RE로 높음               │  │
│   │  로드 속도       │  느림             │  밀리초                     │  │
│   │  개발 난이도     │  높음             │  중간                       │  │
│   │  디버깅          │  어려움           │  bpftool, tracing 지원      │  │
│   │  권한            │  root 필수        │  CAP_BPF로 제한 가능        │  │
│   └───────────────────┴──────────────────┴─────────────────────────────┘  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 1.2. eBPF의 역사

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          eBPF 발전 타임라인                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   1992: BPF 탄생 (Berkeley Packet Filter)                                   │
│         • Steven McCanne와 Van Jacobson이 개발                              │
│         • tcpdump의 패킷 필터링을 위한 가상 머신                             │
│         • 간단한 명령어 세트, 제한된 용도                                    │
│                                                                             │
│   2014: eBPF 등장 (Linux 3.15-3.18)                                         │
│         • Alexei Starovoitov이 주도                                         │
│         • 범용 명령어 세트 (64비트 레지스터 10개)                            │
│         • BPF Maps 도입 (상태 저장)                                         │
│         • JIT 컴파일러 개선                                                 │
│         • 네트워킹을 넘어 트레이싱, 보안으로 확장                            │
│                                                                             │
│   2016: XDP 도입 (Linux 4.8)                                                │
│         • eXpress Data Path                                                 │
│         • 드라이버 레벨에서 패킷 처리 (최고 성능)                            │
│         • DDoS 방어, 로드밸런싱에 활용                                       │
│                                                                             │
│   2018: BTF 도입 (Linux 4.18)                                               │
│         • BPF Type Format                                                   │
│         • CO-RE 가능하게 함                                                 │
│         • libbpf의 핵심                                                     │
│                                                                             │
│   2020: BPF LSM (Linux 5.7)                                                 │
│         • Linux Security Module 훅에 eBPF 연결                              │
│         • 보안 정책 동적 적용                                               │
│                                                                             │
│   2024: Netkit (Linux 6.7)                                                  │
│         • L3 최적화 가상 네트워크 디바이스                                   │
│         • veth보다 효율적인 Pod 네트워킹                                    │
│         • Cilium의 새로운 datapath 옵션                                     │
│                                                                             │
│   현재: 네트워킹, 보안, 관찰성의 핵심 인프라 기술                            │
│         • Cilium, Calico eBPF, Falco, Datadog, Pixie 등                    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

> **💡 팁 (Tip)**: eBPF는 "extended BPF"의 약자이지만, 원래 BPF(classic BPF, cBPF)와는 완전히 다른 기술입니다. 현재 "eBPF"는 그 자체로 고유명사가 되었으며, 더 이상 "확장된 패킷 필터"라는 의미로 사용되지 않습니다.

### 1.3. eBPF 사용 사례

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          eBPF 주요 사용 사례                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   1. 네트워킹 (Cilium, Calico eBPF)                                         │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │  • kube-proxy 대체 (Service 로드밸런싱)                              │  │
│   │  • 네트워크 정책 (L3/L4/L7)                                          │  │
│   │  • 패킷 필터링, NAT                                                   │  │
│   │  • 로드밸런서 (Maglev, DSR)                                          │  │
│   │  • Service Mesh 데이터 플레인 가속                                    │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   2. 관찰성 (Hubble, Pixie, Datadog, Grafana Beyla)                        │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │  • 네트워크 흐름 모니터링 (sidecar 없이)                              │  │
│   │  • 시스템 콜 추적                                                     │  │
│   │  • 애플리케이션 성능 모니터링 (APM)                                   │  │
│   │  • 분산 트레이싱                                                      │  │
│   │  • 자동 계측 (auto-instrumentation)                                  │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   3. 보안 (Falco, Tetragon, Tracee)                                        │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │  • 런타임 위협 탐지                                                   │  │
│   │  • 시스템 콜 감사                                                     │  │
│   │  • 파일 접근 모니터링                                                 │  │
│   │  • 프로세스 실행 추적                                                 │  │
│   │  • 이상 행동 탐지                                                     │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   4. 성능 분석 (bpftrace, perf, Parca)                                     │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │  • CPU 프로파일링                                                     │  │
│   │  • 메모리 분석                                                        │  │
│   │  • I/O 레이턴시 측정                                                  │  │
│   │  • 함수 호출 추적                                                     │  │
│   │  • 커스텀 메트릭 수집                                                 │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 섹션 요약

- **eBPF**: 커널 내에서 안전하게 실행되는 프로그램을 작성하는 기술
- **장점**: 안전성 (Verifier), 이식성 (CO-RE), 동적 로드, 고성능 (JIT)
- **사용 사례**: 네트워킹 (Cilium), 관찰성 (Hubble), 보안 (Falco), 성능 분석

---

## 2. eBPF 아키텍처

### 2.1. eBPF 프로그램 생명주기

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     eBPF 프로그램 실행 과정                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   1. 소스 코드 작성 (C 또는 Rust)                                           │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │  SEC("xdp")                                                          │  │
│   │  int xdp_drop_icmp(struct xdp_md *ctx) {                            │  │
│   │      void *data_end = (void *)(long)ctx->data_end;                  │  │
│   │      void *data = (void *)(long)ctx->data;                          │  │
│   │      struct ethhdr *eth = data;                                     │  │
│   │                                                                       │  │
│   │      if (data + sizeof(*eth) > data_end)                            │  │
│   │          return XDP_PASS;                                            │  │
│   │                                                                       │  │
│   │      if (eth->h_proto == htons(ETH_P_IP)) {                         │  │
│   │          struct iphdr *ip = data + sizeof(*eth);                    │  │
│   │          if ((void *)ip + sizeof(*ip) <= data_end) {                │  │
│   │              if (ip->protocol == IPPROTO_ICMP)                      │  │
│   │                  return XDP_DROP;                                   │  │
│   │          }                                                          │  │
│   │      }                                                               │  │
│   │      return XDP_PASS;                                                │  │
│   │  }                                                                   │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                        │                                                    │
│                        │ clang -target bpf                                  │
│                        ▼                                                    │
│   2. eBPF 바이트코드 (.o ELF 파일)                                         │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │  eBPF 명령어 (64비트 RISC)                                           │  │
│   │                                                                       │  │
│   │  r1 = *(u32 *)(r1 + 4)     // ctx->data_end 로드                    │  │
│   │  r2 = *(u32 *)(r1 + 0)     // ctx->data 로드                        │  │
│   │  r3 = r2 + 14              // eth 헤더 끝                            │  │
│   │  if r3 > r1 goto exit      // 경계 검사                              │  │
│   │  ...                                                                 │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                        │                                                    │
│                        │ bpf() 시스템 콜                                    │
│                        ▼                                                    │
│   3. Verifier 검증                                                         │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │                                                                       │  │
│   │  검사 항목:                                                           │  │
│   │  ✓ 무한 루프 없음 (DAG 검증)                                         │  │
│   │  ✓ 경계 검사 수행됨                                                   │  │
│   │  ✓ NULL 포인터 역참조 없음                                           │  │
│   │  ✓ 초기화되지 않은 메모리 읽기 없음                                   │  │
│   │  ✓ 스택 크기 512바이트 이내                                          │  │
│   │  ✓ Helper 함수 인자 타입 올바름                                      │  │
│   │                                                                       │  │
│   │  결과: 통과 ──► JIT 컴파일                                           │  │
│   │        실패 ──► 로드 거부 (에러 메시지 반환)                          │  │
│   │                                                                       │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                        │ 통과                                               │
│                        ▼                                                    │
│   4. JIT 컴파일                                                            │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │  eBPF 바이트코드 ──► 네이티브 기계어 (x86_64, ARM64)                 │  │
│   │                                                                       │  │
│   │  성능: 인터프리터 대비 ~10배 빠름                                     │  │
│   │  확인: /proc/sys/net/core/bpf_jit_enable = 1                        │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                        │                                                    │
│                        ▼                                                    │
│   5. Hook Point에 부착                                                     │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │  XDP, TC, Socket, Cgroup, Tracepoint 등                              │  │
│   │                                                                       │  │
│   │  ip link set dev eth0 xdp obj drop_icmp.o sec xdp                   │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                        │                                                    │
│                        ▼                                                    │
│   6. 실행                                                                  │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │  패킷 도착 / 이벤트 발생 시 eBPF 프로그램 실행                        │  │
│   │  결과를 커널에 반환 (XDP_DROP, XDP_PASS 등)                          │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```


### 2.2 BPF Verifier: 커널 보호자

BPF Verifier는 eBPF의 안전성을 보장하는 핵심 컴포넌트입니다. 모든 eBPF 프로그램은 커널에 로드되기 전에 반드시 Verifier의 검증을 통과해야 합니다.

> **📘 개념 (Concept)**: Verifier는 "의심스러우면 거부한다"는 원칙을 따릅니다. 이는 커널의 안정성을 최우선으로 보장하기 위함입니다. 유효한 프로그램이 거부될 수는 있어도, 위험한 프로그램이 통과되어서는 안 됩니다.

#### Verifier가 검증하는 항목

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        BPF Verifier 검증 체크리스트                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  1. 제어 흐름 분석 (Control Flow Analysis)                                  │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │  • 모든 실행 경로가 정상적으로 종료하는가?                             │  │
│  │  • 무한 루프가 없는가? (bounded loops만 허용)                         │  │
│  │  • 도달 불가능한 코드가 있는가?                                       │  │
│  │  • 프로그램 크기가 한도 내인가? (1M instructions)                     │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│  2. 메모리 안전성 검증 (Memory Safety)                                      │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │  • 포인터 연산이 유효한 범위 내인가?                                   │  │
│  │  • NULL 포인터 역참조가 없는가?                                       │  │
│  │  • 스택 경계를 벗어나는 접근이 없는가? (512 bytes 스택 한계)          │  │
│  │  • 초기화되지 않은 변수 사용이 없는가?                                 │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│  3. 타입 안전성 검증 (Type Safety)                                          │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │  • 레지스터가 올바른 타입의 값을 담고 있는가?                          │  │
│  │  • 포인터 타입이 올바르게 사용되는가?                                  │  │
│  │  • Helper 함수 인자 타입이 맞는가?                                    │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│  4. 권한 검증 (Privilege Check)                                             │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │  • CAP_BPF 또는 CAP_SYS_ADMIN 권한이 있는가?                          │  │
│  │  • 프로그램 타입에 맞는 권한을 가지고 있는가?                          │  │
│  │  • 접근하려는 리소스에 대한 권한이 있는가?                             │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 검증 과정 예시

```c
// 안전한 eBPF 프로그램 예시
SEC("xdp")
int xdp_pass(struct xdp_md *ctx) {
    void *data = (void *)(long)ctx->data;
    void *data_end = (void *)(long)ctx->data_end;
    
    // Verifier가 요구하는 경계 검사
    // 이 검사 없이 ethhdr에 접근하면 Verifier가 거부
    struct ethhdr *eth = data;
    if (data + sizeof(*eth) > data_end) {
        return XDP_DROP;  // 패킷이 너무 작음
    }
    
    // 이제 안전하게 eth 헤더 접근 가능
    if (eth->h_proto == htons(ETH_P_IP)) {
        // IPv4 처리
    }
    
    return XDP_PASS;
}
```

> **⚠️ 주의 (Warning)**: 경계 검사(bounds check) 없이 패킷 데이터에 접근하면 Verifier가 프로그램을 거부합니다. "R1 offset is outside of the packet"와 같은 에러가 발생합니다.

#### Verifier 에러 디버깅

```bash
# Verifier 로그 확인 (프로그램 로드 시)
$ bpftool prog load drop.o /sys/fs/bpf/drop verbose

# 일반적인 Verifier 에러 메시지
libbpf: prog 'xdp_drop': BPF program load failed: Permission denied
libbpf: prog 'xdp_drop': -- BEGIN PROG LOAD LOG --
R1 offset is outside of the packet
processed 15 insns (limit 1000000)
-- END PROG LOAD LOG --
```

> **💡 팁 (Tip)**: Verifier 에러를 해결하는 핵심은 "항상 경계를 먼저 검사하라"입니다. 포인터를 역참조하기 전에 반드시 해당 메모리가 유효한지 확인해야 합니다.

### 2.3 BPF Maps: 상태 저장소

BPF Maps는 eBPF 프로그램과 사용자 공간 간의 데이터 공유 및 상태 저장을 위한 핵심 데이터 구조입니다.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          BPF Maps 아키텍처                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌──────────────────────────────────────────────────────────────────────┐  │
│   │                          User Space                                   │  │
│   │                                                                       │  │
│   │    ┌─────────────┐           ┌─────────────┐                         │  │
│   │    │ Cilium Agent│           │  bpftool    │                         │  │
│   │    └──────┬──────┘           └──────┬──────┘                         │  │
│   │           │ 읽기/쓰기                │ 읽기/쓰기                       │  │
│   └───────────┼──────────────────────────┼───────────────────────────────┘  │
│               │                          │                                  │
│   ════════════╪══════════════════════════╪═══════════════════════════════   │
│               │      System Call         │                                  │
│               │     (bpf syscall)        │                                  │
│   ════════════╪══════════════════════════╪═══════════════════════════════   │
│               ▼                          ▼                                  │
│   ┌──────────────────────────────────────────────────────────────────────┐  │
│   │                         Kernel Space                                  │  │
│   │                                                                       │  │
│   │                    ┌─────────────────────┐                           │  │
│   │                    │      BPF Map        │                           │  │
│   │                    │   ┌─────┬───────┐   │                           │  │
│   │                    │   │ Key │ Value │   │                           │  │
│   │                    │   ├─────┼───────┤   │                           │  │
│   │                    │   │ IP1 │ cnt:5 │   │◄─── eBPF 프로그램 읽기/쓰기│  │
│   │                    │   ├─────┼───────┤   │                           │  │
│   │                    │   │ IP2 │ cnt:3 │   │                           │  │
│   │                    │   └─────┴───────┘   │                           │  │
│   │                    └─────────────────────┘                           │  │
│   │                              ▲                                        │  │
│   │                              │                                        │  │
│   │    ┌───────────┐    ┌───────┴───────┐    ┌───────────┐              │  │
│   │    │ XDP prog  │    │  TC prog      │    │ Socket    │              │  │
│   │    │           │───►│               │───►│ prog      │              │  │
│   │    └───────────┘    └───────────────┘    └───────────┘              │  │
│   │                                                                       │  │
│   └──────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 주요 Map 타입

| Map 타입 | 용도 | Cilium 활용 |
|----------|------|-------------|
| **BPF_MAP_TYPE_HASH** | Key-Value 해시 테이블 | 연결 추적, 정책 룩업 |
| **BPF_MAP_TYPE_ARRAY** | 고정 크기 배열 | 설정 값, 통계 카운터 |
| **BPF_MAP_TYPE_LRU_HASH** | LRU 퇴거 해시 | NAT 테이블, 세션 테이블 |
| **BPF_MAP_TYPE_LPM_TRIE** | Longest Prefix Match | CIDR 기반 정책 |
| **BPF_MAP_TYPE_PERCPU_HASH** | CPU별 해시 | 락-프리 통계 |
| **BPF_MAP_TYPE_RINGBUF** | 링 버퍼 (이벤트) | Hubble 이벤트 전송 |

> **📘 개념 (Concept)**: BPF Maps는 pinning을 통해 파일시스템에 영구화될 수 있습니다. `/sys/fs/bpf/` 아래에 핀된 맵은 프로그램이 종료되어도 유지되며, 다른 프로그램이 재사용할 수 있습니다.

#### Cilium이 사용하는 주요 Maps

```bash
# Cilium의 BPF Maps 확인
$ bpftool map show | grep cilium
121: lru_hash  name cilium_ct4_glo  flags 0x1
        key 20B  value 64B  max_entries 524288  memlock 51904512B
122: lru_hash  name cilium_ct_any4  flags 0x1
        key 20B  value 64B  max_entries 262144  memlock 25952256B
123: hash  name cilium_lb4_svc  flags 0x1
        key 12B  value 28B  max_entries 65536  memlock 3670016B
124: hash  name cilium_lb4_bac  flags 0x1
        key 8B  value 16B  max_entries 65536  memlock 2621440B
```

| Map 이름 | 용도 | 크기 |
|----------|------|------|
| `cilium_ct4_global` | IPv4 Connection Tracking | 524,288 entries |
| `cilium_ct_any4` | IPv4 CT (any protocol) | 262,144 entries |
| `cilium_lb4_services` | Service VIP → Backend 매핑 | 65,536 entries |
| `cilium_lb4_backends` | Backend Pod 정보 | 65,536 entries |
| `cilium_ipcache` | Identity ↔ IP 매핑 | 512,000 entries |
| `cilium_policy` | 네트워크 정책 규칙 | per-endpoint |

> **🔍 심화 학습 (Deep Dive)**: Cilium은 `cilium_ipcache` 맵을 통해 IP 주소와 Security Identity를 매핑합니다. 이를 통해 패킷 처리 시 즉시 Identity를 조회하여 정책을 적용할 수 있습니다.

#### Map 조작 예시

```bash
# Map 내용 덤프
$ bpftool map dump name cilium_lb4_services
key: 0a c9 00 01 00 00 00 00  bb 01 00 00  
value: 00 00 00 03 00 00 00 00  00 00 00 00 ...

# 사람이 읽기 쉬운 형태로 변환
$ cilium bpf lb list
SERVICE ADDRESS       BACKEND ADDRESS
10.201.0.1:443        10.200.1.15:6443 (active)
                      10.200.2.15:6443 (active)
                      10.200.3.15:6443 (active)
```

### 2.4 JIT 컴파일러: 성능의 핵심

JIT(Just-In-Time) 컴파일러는 eBPF 바이트코드를 네이티브 기계어로 변환하여 성능을 극대화합니다.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      JIT 컴파일 vs 인터프리터                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   인터프리터 방식 (비권장)                                                    │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │  eBPF 바이트코드 ──► 인터프리터 ──► 명령어별 해석/실행               │   │
│   │                                                                      │   │
│   │  패킷마다: fetch → decode → execute → fetch → decode → ...          │   │
│   │  성능: ~10x 느림                                                     │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   JIT 컴파일 방식 (권장)                                                     │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │  eBPF 바이트코드 ──► JIT 컴파일 ──► 네이티브 기계어 (1회)            │   │
│   │                                           │                          │   │
│   │                                           ▼                          │   │
│   │  패킷마다: 네이티브 코드 직접 실행 (CPU가 바로 처리)                  │   │
│   │  성능: 최적                                                          │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### JIT 활성화 확인 및 설정

```bash
# JIT 활성화 상태 확인
$ cat /proc/sys/net/core/bpf_jit_enable
1   # 0=비활성화, 1=활성화, 2=디버그 모드

# JIT 활성화 (영구 설정)
$ echo "net.core.bpf_jit_enable=1" | sudo tee -a /etc/sysctl.d/99-bpf.conf
$ sudo sysctl -p /etc/sysctl.d/99-bpf.conf

# JIT 하드닝 (보안 강화 - constant blinding)
$ cat /proc/sys/net/core/bpf_jit_harden
0   # 0=비활성화, 1=비권한 사용자만, 2=전체
```

> **⚠️ 주의 (Warning)**: `bpf_jit_harden=2`는 보안을 강화하지만 성능이 저하됩니다. 프로덕션 환경에서는 보안 요구사항에 따라 적절히 선택하세요.

#### 아키텍처별 JIT 지원

| 아키텍처 | JIT 지원 | 상태 |
|----------|----------|------|
| x86_64 | ✅ | 완전 지원 (가장 성숙) |
| ARM64 | ✅ | 완전 지원 |
| ARM32 | ✅ | 지원 |
| RISC-V | ✅ | 지원 (Linux 5.1+) |
| LoongArch | ✅ | 지원 (Linux 6.1+) |

### 섹션 요약

이 섹션에서 다룬 eBPF 아키텍처의 핵심 구성요소:

| 구성요소 | 역할 | 핵심 특징 |
|----------|------|-----------|
| **생명주기** | 프로그램 로드~실행 | Verifier → JIT → Hook |
| **Verifier** | 안전성 보장 | 경계 검사, 타입 검증 |
| **BPF Maps** | 상태 저장/공유 | 다양한 타입, User/Kernel 공유 |
| **JIT** | 성능 최적화 | 네이티브 코드, ~10x 성능 |

---

## 3. eBPF Hook Points

eBPF 프로그램은 커널의 다양한 지점(Hook Point)에 부착되어 실행됩니다. Hook Point의 위치에 따라 처리할 수 있는 데이터와 수행할 수 있는 작업이 달라집니다.

### 3.1 패킷 처리 경로와 Hook Points

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                           Linux 네트워크 스택 Hook Points                            │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│   외부 네트워크                                                                      │
│        │                                                                            │
│        ▼                                                                            │
│   ┌─────────────────────────────────────────────────────────────────────────────┐   │
│   │  NIC (Network Interface Card)                                                │   │
│   └─────────────────────────────────────────────────────────────────────────────┘   │
│        │                                                                            │
│        │ 패킷 수신                                                                   │
│        ▼                                                                            │
│   ┌─────────────────────────────────────────────────────────────────────────────┐   │
│   │  ① XDP (eXpress Data Path)                                                   │   │
│   │                                                                               │   │
│   │  위치: 드라이버 레벨 (sk_buff 생성 전)                                        │   │
│   │  장점: 최고 성능, DDoS 방어에 최적                                            │   │
│   │  단점: 기능 제한적, L2 헤더만 접근 가능                                       │   │
│   │                                                                               │   │
│   │  Actions: XDP_DROP, XDP_PASS, XDP_TX, XDP_REDIRECT, XDP_ABORTED              │   │
│   └─────────────────────────────────────────────────────────────────────────────┘   │
│        │ XDP_PASS                                                                   │
│        ▼                                                                            │
│   ┌─────────────────────────────────────────────────────────────────────────────┐   │
│   │  sk_buff 생성 (Socket Buffer)                                                │   │
│   └─────────────────────────────────────────────────────────────────────────────┘   │
│        │                                                                            │
│        ▼                                                                            │
│   ┌─────────────────────────────────────────────────────────────────────────────┐   │
│   │  ② TC (Traffic Control) - Ingress                                            │   │
│   │                                                                               │   │
│   │  위치: 네트워크 스택 진입 직후                                                │   │
│   │  장점: 풍부한 기능, sk_buff 전체 접근                                         │   │
│   │  단점: XDP보다 약간 느림                                                      │   │
│   │                                                                               │   │
│   │  Actions: TC_ACT_OK, TC_ACT_SHOT, TC_ACT_REDIRECT, TC_ACT_PIPE              │   │
│   └─────────────────────────────────────────────────────────────────────────────┘   │
│        │                                                                            │
│        ▼                                                                            │
│   ┌─────────────────────────────────────────────────────────────────────────────┐   │
│   │  Netfilter (iptables/nftables) - PREROUTING                                  │   │
│   └─────────────────────────────────────────────────────────────────────────────┘   │
│        │                                                                            │
│        ├─────────────────────────┐                                                 │
│        │ 라우팅 결정              │                                                 │
│        ▼                         ▼                                                 │
│   ┌─────────────┐         ┌─────────────────────────────────────────┐             │
│   │  Forward    │         │  Local Delivery                         │             │
│   └─────────────┘         │                                         │             │
│        │                   │  ③ Cgroup Hooks                        │             │
│        ▼                   │  ┌───────────────────────────────────┐  │             │
│   ┌─────────────┐         │  │  - cgroup/connect4,6              │  │             │
│   │  TC Egress  │         │  │  - cgroup/bind4,6                 │  │             │
│   └─────────────┘         │  │  - cgroup/sendmsg4,6              │  │             │
│        │                   │  │  - cgroup/recvmsg4,6              │  │             │
│        ▼                   │  │  - cgroup/getsockopt              │  │             │
│   ┌─────────────┐         │  │  - cgroup/setsockopt              │  │             │
│   │    NIC      │         │  └───────────────────────────────────┘  │             │
│   └─────────────┘         │                                         │             │
│        │                   │  ④ Socket Hooks                        │             │
│        ▼                   │  ┌───────────────────────────────────┐  │             │
│   외부 네트워크            │  │  - socket/filter (read filtering) │  │             │
│                            │  │  - sockops (TCP 이벤트 처리)      │  │             │
│                            │  │  - sk_msg (L7 프록시)             │  │             │
│                            │  └───────────────────────────────────┘  │             │
│                            │                                         │             │
│                            │  ⑤ Application                         │             │
│                            │  ┌───────────────────────────────────┐  │             │
│                            │  │  read()/recv()/send()/write()    │  │             │
│                            │  └───────────────────────────────────┘  │             │
│                            └─────────────────────────────────────────┘             │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### 3.2 XDP (eXpress Data Path)

XDP는 가장 빠른 패킷 처리 경로를 제공합니다. 드라이버 레벨에서 패킷을 처리하므로 커널 네트워크 스택의 오버헤드를 완전히 우회합니다.

> **📘 개념 (Concept)**: XDP는 "Zero-Copy" 패킷 처리를 가능하게 합니다. 패킷이 커널 메모리에 복사되기 전에 처리할 수 있어, 고성능 DDoS 방어나 로드 밸런싱에 이상적입니다.

#### XDP 운영 모드

| 모드 | 성능 | 요구사항 | 사용 사례 |
|------|------|----------|-----------|
| **Native XDP** | 최고 | 드라이버 지원 필요 | 프로덕션 LB, DDoS 방어 |
| **Generic XDP** | 낮음 | 모든 NIC | 개발/테스트 |
| **Offloaded XDP** | 극한 | SmartNIC 필요 | 극한 성능 요구 |

#### XDP Actions

```c
// XDP 프로그램이 반환할 수 있는 액션
enum xdp_action {
    XDP_ABORTED = 0,  // 에러 발생, 패킷 드롭 + 에러 추적
    XDP_DROP    = 1,  // 패킷 조용히 드롭 (DDoS 방어)
    XDP_PASS    = 2,  // 커널 스택으로 전달
    XDP_TX      = 3,  // 동일 NIC로 다시 전송 (반사)
    XDP_REDIRECT= 4,  // 다른 NIC 또는 CPU로 전달
};
```

#### XDP 예시: DDoS 방어

```c
#include <linux/bpf.h>
#include <linux/if_ether.h>
#include <linux/ip.h>
#include <bpf/bpf_helpers.h>

// 차단할 IP 목록 (BPF Map)
struct {
    __uint(type, BPF_MAP_TYPE_HASH);
    __uint(max_entries, 10000);
    __type(key, __u32);        // IPv4 주소
    __type(value, __u64);      // 드롭 카운터
} blocked_ips SEC(".maps");

SEC("xdp")
int xdp_ddos_filter(struct xdp_md *ctx) {
    void *data = (void *)(long)ctx->data;
    void *data_end = (void *)(long)ctx->data_end;
    
    // Ethernet 헤더 파싱 (경계 검사 필수!)
    struct ethhdr *eth = data;
    if ((void *)(eth + 1) > data_end)
        return XDP_PASS;
    
    // IPv4만 처리
    if (eth->h_proto != __constant_htons(ETH_P_IP))
        return XDP_PASS;
    
    // IP 헤더 파싱
    struct iphdr *ip = (void *)(eth + 1);
    if ((void *)(ip + 1) > data_end)
        return XDP_PASS;
    
    // 소스 IP가 차단 목록에 있는지 확인
    __u32 src_ip = ip->saddr;
    __u64 *counter = bpf_map_lookup_elem(&blocked_ips, &src_ip);
    
    if (counter) {
        // 차단 목록에 있음 - 드롭하고 카운터 증가
        __sync_fetch_and_add(counter, 1);
        return XDP_DROP;
    }
    
    return XDP_PASS;
}

char _license[] SEC("license") = "GPL";
```

> **💡 팁 (Tip)**: XDP 프로그램은 매우 빨라서 초당 수백만 패킷을 처리할 수 있습니다. 그러나 복잡한 로직은 TC나 Socket 레벨로 넘기는 것이 좋습니다.

### 3.3 TC (Traffic Control)

TC는 XDP보다 더 풍부한 기능을 제공하며, sk_buff 전체에 접근할 수 있습니다.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      TC Hook Points (Ingress/Egress)                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│                          ┌───────────────┐                                  │
│                          │    Network    │                                  │
│                          │   Interface   │                                  │
│                          └───────┬───────┘                                  │
│                                  │                                          │
│              ┌───────────────────┼───────────────────┐                      │
│              │                   │                   │                      │
│              ▼                   │                   ▼                      │
│   ┌─────────────────┐            │        ┌─────────────────┐               │
│   │   TC Ingress    │            │        │   TC Egress     │               │
│   │                 │            │        │                 │               │
│   │  • 패킷 필터링   │            │        │  • 패킷 수정     │               │
│   │  • 패킷 수정     │            │        │  • 트래픽 쉐이핑 │               │
│   │  • 리다이렉트    │            │        │  • NAT          │               │
│   │  • 통계 수집     │            │        │  • 마킹         │               │
│   └────────┬────────┘            │        └────────┬────────┘               │
│            │                     │                 │                        │
│            ▼                     │                 │                        │
│   ┌─────────────────┐            │                 │                        │
│   │  Network Stack  │◄───────────┴─────────────────┘                        │
│   │                 │                                                       │
│   │  (routing,      │                                                       │
│   │   forwarding,   │                                                       │
│   │   local delivery)                                                       │
│   └─────────────────┘                                                       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Cilium의 TC 활용

Cilium은 TC hook을 사용하여:
- **Pod 간 통신**: TC ingress/egress에서 정책 적용
- **서비스 로드밸런싱**: DNAT/SNAT 수행
- **네트워크 정책**: L3/L4 필터링

```bash
# Pod의 TC 프로그램 확인
$ tc filter show dev lxc_health_ep ingress
filter protocol all pref 1 bpf chain 0
filter protocol all pref 1 bpf chain 0 handle 0x1 cil_from_container-lxc_hea \
  direct-action not_in_hw id 3467 tag 85b3f4c8e3e3d0c2 jited
```

### 3.4 Socket Hooks

Socket hooks는 애플리케이션 레벨에서 네트워크 동작을 제어합니다. Cilium의 Socket-based Load Balancing의 핵심입니다.

> **📘 개념 (Concept)**: Socket LB는 `connect()` 시스템 콜을 가로채서 Service IP를 직접 Backend Pod IP로 변환합니다. 이로써 패킷이 전송되기도 전에 로드밸런싱이 완료됩니다.

#### 주요 Socket Hook 타입

| Hook | 트리거 시점 | Cilium 활용 |
|------|-------------|-------------|
| `cgroup/connect4` | `connect()` 호출 | Socket LB (Service → Pod) |
| `cgroup/bind4` | `bind()` 호출 | 포트 제어 |
| `cgroup/sendmsg4` | UDP `sendmsg()` | UDP LB |
| `sockops` | TCP 상태 변화 | Socket Map 관리 |
| `sk_msg` | 소켓 메시지 전송 | L7 정책, 프록시 |

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     Socket-based Load Balancing 흐름                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Pod A (Client)                                                            │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │  Application                                                        │   │
│   │                                                                      │   │
│   │  connect(svc_ip:port)  ───┐                                         │   │
│   │                           │                                         │   │
│   └───────────────────────────┼─────────────────────────────────────────┘   │
│                               │                                             │
│   ┌───────────────────────────▼─────────────────────────────────────────┐   │
│   │  cgroup/connect4 eBPF hook                                          │   │
│   │                                                                      │   │
│   │  1. Service IP 확인: svc_ip → cilium_lb4_services map 조회          │   │
│   │  2. Backend 선택: Maglev 해싱으로 Pod 선택                          │   │
│   │  3. 주소 변환: svc_ip:port → pod_ip:target_port                     │   │
│   │                                                                      │   │
│   │  ctx->user_ip4 = pod_ip;                                            │   │
│   │  ctx->user_port = target_port;                                      │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                               │                                             │
│                               ▼                                             │
│   TCP 3-way handshake가 직접 Pod B로 수행                                   │
│   (Service IP 경유 없음, NAT 없음)                                          │
│                               │                                             │
│                               ▼                                             │
│   Pod B (Backend)                                                           │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │  Application (receiving direct connection from Pod A)               │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 3.5 Cgroup Hooks

Cgroup hooks는 프로세스 그룹 단위로 네트워크 동작을 제어합니다. Kubernetes에서 Pod는 cgroup으로 격리되므로, cgroup hook을 통해 Pod 단위 제어가 가능합니다.

```bash
# Cgroup v2 마운트 확인
$ mount | grep cgroup2
cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot)

# Cilium의 cgroup hook 확인
$ bpftool cgroup tree /sys/fs/cgroup
CgroupPath
ID       AttachType      AttachFlags     Name
/sys/fs/cgroup
5683     connect4                        cil_sock4_connect
5684     connect6                        cil_sock6_connect
5685     sendmsg4                        cil_sock4_sendmsg
5686     recvmsg4                        cil_sock4_recvmsg
5687     getpeername4                    cil_sock4_getpeerna
5688     post_bind4                      cil_sock4_post_bind
```

> **💡 팁 (Tip)**: `bpftool cgroup tree`로 어떤 eBPF 프로그램이 cgroup에 부착되어 있는지 확인할 수 있습니다. 이는 Cilium 디버깅 시 매우 유용합니다.

### 섹션 요약

| Hook Point | 위치 | 성능 | 주요 용도 |
|------------|------|------|-----------|
| **XDP** | 드라이버 레벨 | 최고 | DDoS 방어, 조기 필터링 |
| **TC** | sk_buff 생성 후 | 높음 | 패킷 수정, NAT, 정책 |
| **Socket** | 애플리케이션 레벨 | 높음 | Socket LB, L7 정책 |
| **Cgroup** | 프로세스 그룹 | 높음 | Pod 단위 제어 |

---

## 4. Cilium 아키텍처

Cilium은 eBPF를 활용하여 Kubernetes 네트워킹, 보안, 관찰성을 제공하는 CNI(Container Network Interface) 플러그인입니다.

### 4.1 전체 아키텍처 개요

```
┌───────────────────────────────────────────────────────────────────────────────────────┐
│                              Cilium 아키텍처 개요                                      │
├───────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                       │
│   Control Plane                                                                       │
│   ┌───────────────────────────────────────────────────────────────────────────────┐   │
│   │                          Kubernetes API Server                                 │   │
│   │                                                                                │   │
│   │  ┌─────────────┐  ┌─────────────┐  ┌───────────────────┐  ┌────────────────┐  │   │
│   │  │    Pods     │  │  Services   │  │ NetworkPolicies   │  │ CiliumXXXXX    │  │   │
│   │  └─────────────┘  └─────────────┘  └───────────────────┘  │ (CRDs)         │  │   │
│   │                                                           └────────────────┘  │   │
│   └───────────────────────────────────────────────────────────────────────────────┘   │
│                    │ Watch                           │ Watch                          │
│                    ▼                                 ▼                                │
│   ┌─────────────────────────────┐     ┌─────────────────────────────────────────┐    │
│   │     Cilium Operator         │     │              Cilium Agent               │    │
│   │     (Deployment)            │     │         (DaemonSet - 노드당 1개)         │    │
│   │                             │     │                                         │    │
│   │  • IPAM 관리                │     │  • eBPF 프로그램 로드/관리              │    │
│   │  • CiliumNode 동기화        │◄───►│  • BPF Map 업데이트                     │    │
│   │  • Identity GC              │     │  • Endpoint 관리                        │    │
│   │  • CES 동기화               │     │  • 정책 계산/적용                       │    │
│   │                             │     │  • Hubble 이벤트 수집                   │    │
│   └─────────────────────────────┘     └─────────────────────────────────────────┘    │
│                                                       │                               │
│   ─────────────────────────────────────────────────────────────────────────────────   │
│                                                       │                               │
│   Data Plane                                          ▼                               │
│   ┌───────────────────────────────────────────────────────────────────────────────┐   │
│   │                              eBPF Programs                                     │   │
│   │                                                                                │   │
│   │   ┌────────────┐  ┌────────────┐  ┌────────────┐  ┌────────────┐             │   │
│   │   │    XDP     │  │  TC Hook   │  │  Socket    │  │  Cgroup    │             │   │
│   │   │            │  │            │  │  Hooks     │  │  Hooks     │             │   │
│   │   │ • LB 가속  │  │ • 정책     │  │ • Socket   │  │ • Pod      │             │   │
│   │   │ • DDoS     │  │ • NAT      │  │   LB       │  │   제어     │             │   │
│   │   └────────────┘  └────────────┘  └────────────┘  └────────────┘             │   │
│   │         │               │               │               │                     │   │
│   │         └───────────────┴───────────────┴───────────────┘                     │   │
│   │                                   │                                           │   │
│   │                                   ▼                                           │   │
│   │   ┌───────────────────────────────────────────────────────────────────────┐   │   │
│   │   │                          BPF Maps                                      │   │   │
│   │   │                                                                        │   │   │
│   │   │  ┌──────────────┐ ┌──────────────┐ ┌──────────────┐ ┌──────────────┐  │   │   │
│   │   │  │ cilium_ct4   │ │ cilium_lb4   │ │ cilium_     │ │ cilium_      │  │   │   │
│   │   │  │ _global      │ │ _services    │ │ ipcache     │ │ policy      │  │   │   │
│   │   │  │ (ConnTrack)  │ │ (Services)   │ │ (Identity)  │ │ (Rules)     │  │   │   │
│   │   │  └──────────────┘ └──────────────┘ └──────────────┘ └──────────────┘  │   │   │
│   │   └───────────────────────────────────────────────────────────────────────┘   │   │
│   │                                                                                │   │
│   └───────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                       │
└───────────────────────────────────────────────────────────────────────────────────────┘
```

### 4.2 Cilium Agent

Cilium Agent는 각 노드에서 실행되는 DaemonSet으로, eBPF 데이터플레인을 관리합니다.

> **📘 개념 (Concept)**: Cilium Agent는 Kubernetes API를 감시하여 Pod, Service, NetworkPolicy 변경을 감지하고, 이를 eBPF 프로그램과 BPF Maps에 반영합니다.

#### Agent 주요 기능

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         Cilium Agent 내부 구조                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                          K8s Watcher                                 │   │
│  │  ┌───────────┐ ┌───────────┐ ┌───────────┐ ┌───────────┐           │   │
│  │  │   Pods    │ │  Services │ │ Endpoints │ │  Policies │           │   │
│  │  └─────┬─────┘ └─────┬─────┘ └─────┬─────┘ └─────┬─────┘           │   │
│  └────────┼─────────────┼─────────────┼─────────────┼───────────────────┘   │
│           │             │             │             │                       │
│           ▼             ▼             ▼             ▼                       │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                       Event Queue & Processing                       │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│           │                                                                 │
│           ├─────────────────┬─────────────────┬─────────────────┐          │
│           ▼                 ▼                 ▼                 ▼          │
│  ┌─────────────┐   ┌─────────────┐   ┌─────────────┐   ┌─────────────┐    │
│  │  Endpoint   │   │  Service    │   │  Identity   │   │   Policy    │    │
│  │  Manager    │   │  Manager    │   │  Manager    │   │   Engine    │    │
│  │             │   │             │   │             │   │             │    │
│  │ • Pod 엔드  │   │ • ClusterIP │   │ • Security  │   │ • L3/L4/L7  │    │
│  │   포인트    │   │ • NodePort  │   │   Identity  │   │   정책 계산 │    │
│  │ • IP 할당   │   │ • LB 설정   │   │   할당/관리 │   │ • BPF Map   │    │
│  │ • veth 생성 │   │             │   │             │   │   업데이트  │    │
│  └──────┬──────┘   └──────┬──────┘   └──────┬──────┘   └──────┬──────┘    │
│         │                 │                 │                 │            │
│         └─────────────────┴─────────────────┴─────────────────┘            │
│                                     │                                       │
│                                     ▼                                       │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                        BPF Datapath Manager                          │   │
│  │                                                                      │   │
│  │  • eBPF 프로그램 컴파일/로드                                         │   │
│  │  • BPF Map 생성/업데이트                                             │   │
│  │  • TC/XDP Hook 관리                                                  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Agent 상태 확인

```bash
# Agent 상태 확인
$ cilium status
    /¯¯\
 /¯¯\__/¯¯\    Cilium:             OK
 \__/¯¯\__/    Operator:           OK
 /¯¯\__/¯¯\    Envoy DaemonSet:    disabled
 \__/¯¯\__/    Hubble Relay:       OK
    \__/       ClusterMesh:        disabled

DaemonSet              cilium             Desired: 3, Ready: 3/3, Available: 3/3
Deployment             cilium-operator    Desired: 1, Ready: 1/1, Available: 1/1
Deployment             hubble-relay       Desired: 1, Ready: 1/1, Available: 1/1
Containers:            cilium             Running: 3
                       cilium-operator    Running: 1
                       hubble-relay       Running: 1

# 노드의 BPF 프로그램 목록
$ cilium bpf prog list
Type       Section                                      Pinned
xdp        cil_xdp_entry                               /sys/fs/bpf/tc/globals/cil_xdp_entry
tc/ingress cil_from_container                          -
tc/egress  cil_to_container                            -
```

### 4.3 Cilium Operator

Cilium Operator는 클러스터 수준의 작업을 처리합니다.

| 기능 | 설명 |
|------|------|
| **IPAM 관리** | 노드에 Pod CIDR 할당 |
| **Identity GC** | 사용하지 않는 Security Identity 정리 |
| **CiliumNode 동기화** | 노드 정보 Kubernetes와 동기화 |
| **CES 동기화** | CiliumEndpointSlice 관리 (대규모 클러스터) |

```bash
# Operator 로그 확인
$ kubectl logs -n kube-system deployment/cilium-operator

# IPAM 할당 상태 확인
$ kubectl get ciliumnodes -o wide
NAME    CILIUMINTERNALIP   ALLOCATED-IPS   ALLOCATION-POOL
node1   10.200.1.1         45              10.200.1.0/24
node2   10.200.2.1         38              10.200.2.0/24
node3   10.200.3.1         52              10.200.3.0/24
```

> **🔍 심화 학습 (Deep Dive)**: Cilium Operator는 기본적으로 1개만 실행됩니다 (Leader Election). 고가용성을 위해 `operator.replicas=2`로 설정할 수 있지만, 한 번에 하나만 활성화됩니다.

### 4.4 Cilium CRDs (Custom Resource Definitions)

Cilium은 다양한 CRD를 통해 기능을 확장합니다.

```bash
# Cilium CRD 목록
$ kubectl get crd | grep cilium
ciliumclusterwidenetworkpolicies.cilium.io      2024-01-15T10:00:00Z
ciliumendpoints.cilium.io                        2024-01-15T10:00:00Z
ciliumidentities.cilium.io                       2024-01-15T10:00:00Z
ciliumnetworkpolicies.cilium.io                  2024-01-15T10:00:00Z
ciliumnodes.cilium.io                            2024-01-15T10:00:00Z
ciliumloadbalancerippools.cilium.io             2024-01-15T10:00:00Z
ciliumbgppeeringpolicies.cilium.io              2024-01-15T10:00:00Z
```

#### 주요 CRD

| CRD | 용도 | 예시 |
|-----|------|------|
| **CiliumNetworkPolicy** | L3/L4/L7 네트워크 정책 | Pod 간 통신 제어 |
| **CiliumClusterwideNetworkPolicy** | 클러스터 전체 정책 | Namespace 무관 정책 |
| **CiliumEndpoint** | Pod 엔드포인트 정보 | IP, Identity, 정책 상태 |
| **CiliumIdentity** | Security Identity 정의 | 라벨 → Identity 매핑 |
| **CiliumNode** | 노드 정보 | IPAM, Routing 정보 |
| **CiliumLoadBalancerIPPool** | LB IP 풀 | MetalLB 대체 |
| **CiliumBGPPeeringPolicy** | BGP 피어링 설정 | 외부 라우터 연동 |

### 4.5 Endpoint와 Identity

Cilium은 Pod를 **Endpoint**로 추상화하고, 동일한 라벨을 가진 Pod들에게 **Security Identity**를 부여합니다.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     Endpoint와 Security Identity 관계                        │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Pods with same labels → Same Security Identity                            │
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │  Namespace: production                                               │   │
│   │                                                                      │   │
│   │   Pod A (web-frontend)          Pod B (web-frontend)                 │   │
│   │   ┌─────────────────────┐       ┌─────────────────────┐             │   │
│   │   │ labels:             │       │ labels:             │             │   │
│   │   │   app: frontend     │       │   app: frontend     │             │   │
│   │   │   tier: web         │       │   tier: web         │             │   │
│   │   │                     │       │                     │             │   │
│   │   │ Endpoint ID: 1234   │       │ Endpoint ID: 1235   │             │   │
│   │   │ IP: 10.200.1.45     │       │ IP: 10.200.2.67     │             │   │
│   │   └──────────┬──────────┘       └──────────┬──────────┘             │   │
│   │              │                             │                         │   │
│   │              └──────────┬──────────────────┘                         │   │
│   │                         │                                            │   │
│   │                         ▼                                            │   │
│   │              ┌─────────────────────┐                                 │   │
│   │              │  Security Identity  │                                 │   │
│   │              │       ID: 54321     │                                 │   │
│   │              │                     │                                 │   │
│   │              │  k8s:app=frontend   │                                 │   │
│   │              │  k8s:tier=web       │                                 │   │
│   │              │  k8s:ns=production  │                                 │   │
│   │              └─────────────────────┘                                 │   │
│   │                                                                      │   │
│   └──────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   → 정책은 Identity 단위로 적용되므로, 동일 Identity Pod들은 같은 정책 적용   │
│   → BPF Map에서 IP → Identity 조회로 빠른 정책 결정                          │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

```bash
# Endpoint 목록 확인
$ cilium endpoint list
ENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS (source:key[=value])
1234       Enabled            Enabled           54321      k8s:app=frontend
                                                           k8s:tier=web
                                                           k8s:io.kubernetes.pod.namespace=production

# Identity 확인
$ cilium identity list
ID      LABELS
1       reserved:host
2       reserved:world
3       reserved:unmanaged
4       reserved:health
54321   k8s:app=frontend
        k8s:tier=web
        k8s:io.kubernetes.pod.namespace=production
```

> **📘 개념 (Concept)**: Security Identity는 수치(numeric) ID입니다. 패킷 처리 시 라벨 문자열을 비교하는 것보다 숫자 비교가 훨씬 빠르므로, eBPF에서 효율적인 정책 결정이 가능합니다.

### 섹션 요약

| 구성요소 | 역할 | 배포 형태 |
|----------|------|-----------|
| **Agent** | eBPF 데이터플레인 관리 | DaemonSet (노드당 1개) |
| **Operator** | 클러스터 수준 작업 | Deployment (1-2개) |
| **CRDs** | 확장 리소스 정의 | API 확장 |
| **Endpoint** | Pod 추상화 | Pod당 1개 |
| **Identity** | 보안 그룹화 | 라벨 조합당 1개 |

---

## 5. kube-proxy 대체

Cilium은 kube-proxy를 완전히 대체하여 eBPF 기반의 고성능 서비스 로드밸런싱을 제공합니다.

### 5.1 kube-proxy의 문제점

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        kube-proxy (iptables 모드) 문제점                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Service 개수 증가에 따른 iptables 규칙 증가                                │
│                                                                             │
│   Services: 100개                     Services: 10,000개                    │
│   ┌─────────────────────────────┐    ┌─────────────────────────────┐       │
│   │  iptables rules: ~500       │    │  iptables rules: ~50,000    │       │
│   │                             │    │                             │       │
│   │  패킷당 평균 검사: ~250     │    │  패킷당 평균 검사: ~25,000  │       │
│   │  지연: 낮음                 │    │  지연: 높음                 │       │
│   │                             │    │                             │       │
│   │  O(n) 순차 탐색             │    │  O(n) 순차 탐색             │       │
│   └─────────────────────────────┘    └─────────────────────────────┘       │
│                                                                             │
│   문제점 상세:                                                               │
│                                                                             │
│   1. 선형 탐색 복잡도                                                        │
│      ┌────────────────────────────────────────────────────────────────┐     │
│      │  패킷 → rule1 → rule2 → rule3 → ... → ruleN → ACCEPT/DROP     │     │
│      │         miss     miss     miss           hit                   │     │
│      │                                                                │     │
│      │  평균: N/2 규칙 탐색 필요                                      │     │
│      └────────────────────────────────────────────────────────────────┘     │
│                                                                             │
│   2. conntrack 테이블 크기 제한                                             │
│      - 기본 최대: 262,144 entries                                          │
│      - 대규모 클러스터에서 오버플로우 위험                                  │
│      - 오버플로우 시 새 연결 실패                                           │
│                                                                             │
│   3. 규칙 업데이트 비용                                                      │
│      - Service/Endpoint 변경 시 전체 규칙 재작성                            │
│      - 대규모 클러스터에서 수 초 소요                                       │
│      - 업데이트 중 패킷 손실 가능                                           │
│                                                                             │
│   4. 불필요한 네트워크 스택 순회                                             │
│      - 모든 패킷이 Netfilter 통과                                          │
│      - PREROUTING → FORWARD → POSTROUTING                                  │
│      - 각 단계에서 규칙 검사                                                │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

> **⚠️ 주의 (Warning)**: 대규모 Kubernetes 클러스터(1,000+ Services)에서 kube-proxy의 iptables 모드는 심각한 성능 저하를 야기할 수 있습니다. CPU 사용률 증가, 지연 시간 증가, 간헐적 연결 실패가 발생합니다.

### 5.2 Cilium의 eBPF 기반 서비스 구현

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     Cilium eBPF 서비스 로드밸런싱                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Services: 10,000개                                                        │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                                                                      │   │
│   │  BPF Hash Map 조회: O(1)                                            │   │
│   │                                                                      │   │
│   │  패킷 → Hash(dst_ip:port) → Map Lookup → Backend → 전달            │   │
│   │                                 │                                    │   │
│   │                             단일 연산                                │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   성능 비교:                                                                 │
│   ┌────────────────────────────────────────────────────────────────────┐    │
│   │                                                                     │    │
│   │  구분          │ iptables (kube-proxy) │ Cilium eBPF              │    │
│   │  ─────────────┼───────────────────────┼─────────────────────────  │    │
│   │  복잡도        │ O(n)                  │ O(1)                     │    │
│   │  10K svc 지연  │ ~수 ms                │ ~수 µs                   │    │
│   │  규칙 업데이트 │ 전체 재작성           │ Map entry만 변경        │    │
│   │  conntrack    │ 필수                  │ 최소화 가능             │    │
│   │  CPU 사용     │ 높음                  │ 낮음                     │    │
│   │                                                                     │    │
│   └────────────────────────────────────────────────────────────────────┘    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 5.3 Socket-based Load Balancing

Socket LB는 Cilium의 가장 혁신적인 기능 중 하나입니다.

> **📘 개념 (Concept)**: Socket LB는 애플리케이션의 `connect()` 시스템 콜을 가로채서, Service IP를 직접 Backend Pod IP로 변환합니다. 패킷이 전송되기도 전에 로드밸런싱이 완료되므로 NAT가 불필요합니다.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    기존 방식 vs Socket LB 비교                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   기존 방식 (DNAT 기반)                                                      │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                                                                      │   │
│   │   Pod A                    Network Stack                  Pod B     │   │
│   │   ┌─────┐                                                ┌─────┐   │   │
│   │   │     │ ──── connect(svc_ip) ────►                     │     │   │   │
│   │   │     │                           │                    │     │   │   │
│   │   │     │      ┌────────────────────▼────────────────┐   │     │   │   │
│   │   │     │      │   1. 패킷 생성: src=podA, dst=svcIP │   │     │   │   │
│   │   │     │      │   2. TC/iptables DNAT               │   │     │   │   │
│   │   │     │      │   3. conntrack entry 생성           │   │     │   │   │
│   │   │     │      │   4. 패킷 수정: dst=podB            │   │     │   │   │
│   │   │     │      │   5. 라우팅                          │   │     │   │   │
│   │   │     │      │   6. 전달                            │   │     │   │   │
│   │   │     │      └─────────────────────────────────────┘   │     │   │   │
│   │   │     │                                          ─────►│     │   │   │
│   │   └─────┘                                                └─────┘   │   │
│   │                                                                      │   │
│   │   단점: NAT 오버헤드, conntrack 필요, 복잡한 패킷 경로               │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   Socket LB 방식                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                                                                      │   │
│   │   Pod A                    cgroup eBPF hook              Pod B      │   │
│   │   ┌─────┐                                                ┌─────┐   │   │
│   │   │     │ ──── connect(svc_ip) ────►                     │     │   │   │
│   │   │     │                           │                    │     │   │   │
│   │   │     │      ┌────────────────────▼────────────────┐   │     │   │   │
│   │   │     │      │  cgroup/connect4 hook:              │   │     │   │   │
│   │   │     │      │    svc_ip → backend_ip 변환         │   │     │   │   │
│   │   │     │      │    (connect 인자 직접 수정)         │   │     │   │   │
│   │   │     │      └─────────────────────────────────────┘   │     │   │   │
│   │   │     │                           │                    │     │   │   │
│   │   │     │ ◄─── connect(pod_b_ip) ───┘                    │     │   │   │
│   │   │     │                                                │     │   │   │
│   │   │     │ ════ TCP handshake 직접 수행 ═════════════════►│     │   │   │
│   │   └─────┘                                                └─────┘   │   │
│   │                                                                      │   │
│   │   장점: NAT 없음, conntrack 최소화, 직접 연결                        │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Socket LB 활성화 (cilium-values-custom.yaml)

```yaml
# Socket-Based Load Balancing 설정
socketLB:
  # Socket LB 활성화
  enabled: true
  # 비루트 네임스페이스에서 Socket LB 비활성화
  # Istio 라우팅 규칙과 함께 사용할 경우 true로 설정
  hostNamespaceOnly: false
  # 삭제된 서비스 백엔드에 대한 Pod 연결 종료
  terminatePodConnections: true

# kube-proxy 완전 대체
kubeProxyReplacement: "true"
```

> **💡 팁 (Tip)**: `hostNamespaceOnly: true`로 설정하면 Host 네트워크를 사용하는 Pod에서만 Socket LB가 적용됩니다. Istio와 같은 서비스 메시와 함께 사용할 때 유용합니다.

### 5.4 서비스 BPF Maps

```bash
# Service 맵 확인
$ cilium bpf lb list
SERVICE ADDRESS       BACKEND ADDRESS
10.201.0.1:443        10.200.1.15:6443 (active)
                      10.200.2.15:6443 (active)  
                      10.200.3.15:6443 (active)
10.201.0.10:53        10.200.1.23:53 (active)
                      10.200.2.24:53 (active)
10.201.45.123:80      10.200.1.89:8080 (active)
                      10.200.2.90:8080 (active)
                      10.200.3.91:8080 (active)

# Backend 상세 정보
$ cilium bpf lb list -o json | jq '.[] | select(.address == "10.201.45.123:80")'
{
  "address": "10.201.45.123:80",
  "backends": [
    {
      "address": "10.200.1.89:8080",
      "state": "active",
      "weight": 1
    },
    {
      "address": "10.200.2.90:8080", 
      "state": "active",
      "weight": 1
    }
  ]
}
```

### 섹션 요약

| 기능 | kube-proxy (iptables) | Cilium eBPF |
|------|----------------------|-------------|
| 복잡도 | O(n) | O(1) |
| 구현 | iptables 규칙 | BPF Hash Map |
| NAT | 필수 (DNAT/SNAT) | Socket LB로 우회 가능 |
| conntrack | 모든 연결 추적 | 최소화 가능 |
| 업데이트 | 전체 규칙 재작성 | Map entry만 변경 |

---

## 6. Datapath 모드

Cilium은 Pod 네트워크 인터페이스로 veth와 netkit 두 가지 모드를 지원합니다.

### 6.1 veth vs netkit 비교

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        veth vs netkit 아키텍처 비교                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   veth (기존 방식)                          netkit (Linux 6.7+)             │
│   ┌───────────────────────────────────┐    ┌───────────────────────────────┐│
│   │                                   │    │                               ││
│   │   ┌─────────┐     ┌─────────┐    │    │   ┌─────────┐   ┌─────────┐  ││
│   │   │  Pod    │     │  Host   │    │    │   │  Pod    │   │  Host   │  ││
│   │   │ Network │     │ Network │    │    │   │ Network │   │ Network │  ││
│   │   │   NS    │     │   NS    │    │    │   │   NS    │   │   NS    │  ││
│   │   └────┬────┘     └────┬────┘    │    │   └────┬────┘   └────┬────┘  ││
│   │        │               │         │    │        │             │        ││
│   │   ┌────┴────┐     ┌────┴────┐    │    │   ┌────┴─────────────┴────┐   ││
│   │   │  eth0   │◄───►│  lxcXXX │    │    │   │       netkit          │   ││
│   │   │ (veth)  │     │ (veth)  │    │    │   │    (peer device)      │   ││
│   │   └─────────┘     └─────────┘    │    │   └───────────────────────┘   ││
│   │                                   │    │                               ││
│   │   TC hook으로 BPF 부착            │    │   Native BPF 부착             ││
│   │                                   │    │                               ││
│   │   ┌─────────────────────────────┐│    │   ┌─────────────────────────┐ ││
│   │   │ 패킷 경로:                   ││    │   │ 패킷 경로:               │ ││
│   │   │ 1. Pod eth0 → L2 처리       ││    │   │ 1. Pod → BPF 직접 실행  │ ││
│   │   │ 2. NS 전환                   ││    │   │ 2. NS 전환 (최적화)     │ ││
│   │   │ 3. Host lxc → TC ingress    ││    │   │ 3. Host로 전달          │ ││
│   │   │ 4. BPF 실행                  ││    │   │                         │ ││
│   │   │ 5. 라우팅                    ││    │   │ L2 처리 생략 (L3 모드)  │ ││
│   │   └─────────────────────────────┘│    │   └─────────────────────────┘ ││
│   │                                   │    │                               ││
│   └───────────────────────────────────┘    └───────────────────────────────┘│
│                                                                             │
│   성능 비교:                                                                 │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                                                                      │   │
│   │   항목              │ veth                │ netkit                  │   │
│   │   ─────────────────┼─────────────────────┼─────────────────────────│   │
│   │   L2 처리           │ 필요                │ 불필요 (L3 모드)        │   │
│   │   TC hook 오버헤드  │ 있음                │ 없음 (Native BPF)       │   │
│   │   NS 전환 최적화    │ 기본                │ 최적화됨                │   │
│   │   처리량            │ 기준                │ ~10-15% 향상            │   │
│   │   지연시간          │ 기준                │ ~5-10% 감소             │   │
│   │   최소 커널         │ 4.19+               │ 6.7+                    │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 6.2 netkit 설정

```yaml
# cilium-values-custom.yaml
bpf:
  # datapathMode 옵션:
  # - veth: 전통적인 Linux 가상 이더넷 페어, L2 모드, TC를 통한 BPF
  # - netkit: Linux 6.7+에서 도입된 새로운 가상 디바이스, L3 모드, 네이티브 BPF
  #           네임스페이스 전환 최적화, 성능 향상, 권장됨
  # - netkit-l2: netkit의 L2 모드, 특정 L2 프로토콜 요구사항이 있는 경우 사용
  datapathMode: netkit
```

> **📘 개념 (Concept)**: netkit은 Linux 6.7에서 도입된 새로운 가상 네트워크 디바이스입니다. veth와 달리 L3 레이어에서 직접 작동하여 L2 처리 오버헤드를 제거합니다.

### 6.3 커널 버전 요구사항

| 기능 | 최소 커널 | RHEL 10 (6.12) |
|------|----------|----------------|
| Cilium 기본 | 4.19+ | ✅ 지원 |
| veth 모드 | 4.19+ | ✅ 지원 |
| netkit 모드 | 6.7+ | ✅ 지원 |
| netkit-l2 | 6.7+ | ✅ 지원 |

```bash
# 커널 버전 확인
$ uname -r
6.12.0-55.el10.x86_64

# netkit 지원 확인
$ cat /boot/config-$(uname -r) | grep CONFIG_NETKIT
CONFIG_NETKIT=y
```

> **💡 팁 (Tip)**: RHEL 10 (커널 6.12)에서는 netkit 모드를 사용하는 것을 권장합니다. 성능이 향상되며 Cilium의 최신 기능을 활용할 수 있습니다.

### 섹션 요약

| 모드 | 레이어 | 커널 요구 | 성능 | 권장 사용 |
|------|--------|-----------|------|-----------|
| **veth** | L2 | 4.19+ | 기준 | 레거시 커널 |
| **netkit** | L3 | 6.7+ | +10-15% | **권장** (RHEL 10) |
| **netkit-l2** | L2 | 6.7+ | +5-10% | L2 프로토콜 필요 시 |

---

## 7. 로드 밸런싱

Cilium은 다양한 로드 밸런싱 알고리즘과 모드를 제공합니다.

### 7.1 Maglev 일관된 해싱

> **📘 개념 (Concept)**: Maglev는 Google이 개발한 일관된 해싱(Consistent Hashing) 알고리즘입니다. Backend가 추가/제거되어도 기존 연결의 대부분이 동일한 Backend로 유지됩니다.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         Maglev 일관된 해싱 원리                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   기존 라운드로빈 vs Maglev                                                  │
│                                                                             │
│   Round Robin (Backend 1개 제거 시)                                         │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                                                                      │   │
│   │   변경 전:  Client1→B1, Client2→B2, Client3→B3, Client4→B1          │   │
│   │                                                                      │   │
│   │   B2 제거 후: Client1→B1, Client2→B3, Client3→B1, Client4→B3        │   │
│   │                          ↑           ↑           ↑                   │   │
│   │                     모든 매핑 변경됨                                  │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   Maglev (Backend 1개 제거 시)                                              │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                                                                      │   │
│   │   변경 전:  Client1→B1, Client2→B2, Client3→B3, Client4→B1          │   │
│   │                                                                      │   │
│   │   B2 제거 후: Client1→B1, Client2→B3, Client3→B3, Client4→B1        │   │
│   │                          ↑                                           │   │
│   │                     B2만 재분배됨 (최소 변경)                         │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   Maglev 룩업 테이블 구조                                                    │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                                                                      │   │
│   │   Table Size: 65537 (소수)                                          │   │
│   │                                                                      │   │
│   │   Index:   0    1    2    3    4   ...  65536                       │   │
│   │           ┌────┬────┬────┬────┬────┬────┬─────┐                     │   │
│   │   Backend │ B1 │ B3 │ B2 │ B1 │ B2 │....│ B3  │                     │   │
│   │           └────┴────┴────┴────┴────┴────┴─────┘                     │   │
│   │                                                                      │   │
│   │   Hash(5-tuple) mod 65537 → Index → Backend                         │   │
│   │                                                                      │   │
│   │   예: Hash(10.1.1.1:32145 → 10.201.0.50:80) = 12345                 │   │
│   │       12345 mod 65537 = 12345 → Table[12345] = B2                   │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Maglev 설정

```yaml
# cilium-values-custom.yaml
loadBalancer:
  # 로드 밸런싱 알고리즘
  # - random: 랜덤 선택
  # - maglev: Maglev 해싱 알고리즘 (일관성 있는 해싱, 권장)
  algorithm: maglev
```

### 7.2 DSR (Direct Server Return)

> **📘 개념 (Concept)**: DSR 모드에서는 응답 패킷이 로드밸런서를 거치지 않고 Backend에서 Client로 직접 전송됩니다. 응답 트래픽이 많은 워크로드에서 효과적입니다.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          DSR vs SNAT 비교                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   SNAT 모드 (기본)                                                           │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                                                                      │   │
│   │   Client ─────────► LB ─────────► Backend                           │   │
│   │      │    (request)    (DNAT)                                       │   │
│   │      │                                                               │   │
│   │      │◄───────────── LB ◄─────────── Backend                        │   │
│   │           (response)    (SNAT)       (response)                     │   │
│   │                                                                      │   │
│   │   모든 트래픽이 LB를 통과 → LB가 병목                                 │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   DSR 모드                                                                   │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                                                                      │   │
│   │   Client ─────────► LB ─────────► Backend                           │   │
│   │      │    (request)    (DNAT)                                       │   │
│   │      │                                                               │   │
│   │      │◄────────────────────────────── Backend                       │   │
│   │           (direct response)                                          │   │
│   │                                                                      │   │
│   │   응답은 Backend에서 Client로 직접 전송                               │   │
│   │   LB 부하 감소, 응답 지연 감소                                        │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   Hybrid 모드 (권장)                                                         │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                                                                      │   │
│   │   로컬 Backend → DSR (동일 노드 내 통신)                             │   │
│   │   원격 Backend → SNAT (다른 노드로의 통신)                           │   │
│   │                                                                      │   │
│   │   장점: 복잡한 네트워크 설정 없이 DSR의 이점 활용                     │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### DSR 설정

```yaml
# cilium-values-custom.yaml
loadBalancer:
  # 원격 백엔드에 대한 로드 밸런싱 모드
  # - snat: Source NAT 모드 (기본)
  # - dsr: Direct Server Return 모드 (성능 최적화)
  # - hybrid: 로컬 백엔드는 DSR, 원격 백엔드는 SNAT (권장)
  mode: hybrid
```

> **⚠️ 주의 (Warning)**: 순수 DSR 모드는 네트워크 토폴로지에 따라 작동하지 않을 수 있습니다. Backend 노드가 Client에게 직접 응답을 보낼 수 있어야 합니다. 대부분의 환경에서는 hybrid 모드를 권장합니다.

### 7.3 XDP 가속화

XDP를 사용하면 로드밸런싱을 더욱 가속화할 수 있습니다.

```yaml
# cilium-values-custom.yaml
loadBalancer:
  # XDP 가속화 설정
  # - disabled: XDP 사용 안 함
  # - native: 네이티브 XDP 모드 (최고 성능, 드라이버 지원 필요)
  # - best-effort: 지원되는 디바이스에서만 네이티브 XDP 사용 (권장)
  acceleration: best-effort
```

```bash
# XDP 가속화 상태 확인
$ cilium status | grep -A3 "KubeProxyReplacement"
KubeProxyReplacement:   True
  Devices:              eth0 (XDP native mode)
  Mode:                 SNAT, DSR Hybrid
  Algorithm:            Maglev
```

### 섹션 요약

| 기능 | 옵션 | 권장 설정 |
|------|------|-----------|
| **알고리즘** | random, maglev | `maglev` |
| **모드** | snat, dsr, hybrid | `hybrid` |
| **XDP 가속** | disabled, native, best-effort | `best-effort` |

---

## 8. 네트워크 정책

Cilium은 Kubernetes NetworkPolicy를 확장한 CiliumNetworkPolicy를 통해 L3/L4/L7 수준의 정책을 제공합니다.

### 8.1 정책 계층

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                       Cilium 네트워크 정책 계층                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │  L7 정책 (Application Layer)                                        │   │
│   │                                                                      │   │
│   │  • HTTP Method/Path 필터링                                          │   │
│   │  • gRPC 메서드 제어                                                  │   │
│   │  • Kafka Topic 접근 제어                                             │   │
│   │  • DNS 쿼리 필터링                                                   │   │
│   │                                                                      │   │
│   │  예: GET /api/v1/users 만 허용                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                           │                                                 │
│                           ▼                                                 │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │  L4 정책 (Transport Layer)                                          │   │
│   │                                                                      │   │
│   │  • TCP/UDP 포트 필터링                                               │   │
│   │  • ICMP 타입/코드 제어                                               │   │
│   │                                                                      │   │
│   │  예: TCP 포트 80, 443만 허용                                        │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                           │                                                 │
│                           ▼                                                 │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │  L3 정책 (Network Layer)                                            │   │
│   │                                                                      │   │
│   │  • IP 주소/CIDR 필터링                                               │   │
│   │  • Kubernetes Label 기반 선택                                        │   │
│   │  • Security Identity 기반 선택                                       │   │
│   │                                                                      │   │
│   │  예: app=backend 라벨을 가진 Pod에서만 접근 허용                     │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 8.2 CiliumNetworkPolicy 예시

#### L3/L4 정책

```yaml
apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
metadata:
  name: "allow-frontend-to-backend"
  namespace: production
spec:
  # 정책이 적용될 대상
  endpointSelector:
    matchLabels:
      app: backend
  
  # Ingress 규칙 (들어오는 트래픽)
  ingress:
    - fromEndpoints:
        - matchLabels:
            app: frontend
      toPorts:
        - ports:
            - port: "8080"
              protocol: TCP
```

> **📝 예제 (Example)**: 위 정책은 `app: backend` 라벨을 가진 Pod에 대해, `app: frontend` 라벨을 가진 Pod에서 오는 TCP 8080 포트 트래픽만 허용합니다.

#### L7 HTTP 정책

```yaml
apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
metadata:
  name: "l7-http-policy"
  namespace: production
spec:
  endpointSelector:
    matchLabels:
      app: api-server
  
  ingress:
    - fromEndpoints:
        - matchLabels:
            app: web-frontend
      toPorts:
        - ports:
            - port: "8080"
              protocol: TCP
          rules:
            http:
              - method: "GET"
                path: "/api/v1/users"
              - method: "GET"
                path: "/api/v1/products"
              - method: "POST"
                path: "/api/v1/orders"
                headers:
                  - 'Content-Type: application/json'
```

> **🔍 심화 학습 (Deep Dive)**: L7 정책은 Envoy 프록시를 사용합니다. Cilium Agent가 자동으로 Envoy를 Pod와 연결하여 L7 트래픽을 검사합니다. 이는 추가적인 지연을 발생시키므로 필요한 경우에만 사용하세요.

#### DNS 기반 정책

```yaml
apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
metadata:
  name: "allow-external-api"
  namespace: production
spec:
  endpointSelector:
    matchLabels:
      app: payment-service
  
  egress:
    - toEndpoints:
        - matchLabels:
            io.kubernetes.pod.namespace: kube-system
            k8s-app: kube-dns
      toPorts:
        - ports:
            - port: "53"
              protocol: UDP
          rules:
            dns:
              - matchPattern: "*.stripe.com"
              - matchPattern: "*.paypal.com"
    
    - toFQDNs:
        - matchName: "api.stripe.com"
        - matchName: "api.paypal.com"
      toPorts:
        - ports:
            - port: "443"
              protocol: TCP
```

> **💡 팁 (Tip)**: DNS 기반 정책을 사용하면 외부 서비스에 대한 접근을 FQDN으로 제어할 수 있습니다. Cilium은 DNS 응답을 모니터링하여 IP 주소를 자동으로 학습합니다.

### 8.3 정책 디버깅

```bash
# Endpoint의 정책 상태 확인
$ cilium endpoint list
ENDPOINT   POLICY (ingress)   POLICY (egress)   IDENTITY   LABELS
1234       Enabled            Enabled           54321      k8s:app=backend

# 정책 적용 상세 확인
$ cilium endpoint get 1234 | jq '.status.policy'
{
  "ingress": {
    "allowed": [
      {
        "identity": 12345,
        "labels": ["k8s:app=frontend"]
      }
    ],
    "denied": []
  },
  "egress": {...}
}

# 정책으로 인해 드롭된 패킷 확인
$ cilium monitor --type drop
level=info msg="Packet dropped" identity=12345 reason="Policy denied"
```

### 8.4 기본 정책 모드

| 모드 | 설명 | 설정 |
|------|------|------|
| **Default Allow** | 정책이 없으면 모두 허용 | `policyEnforcementMode: default` |
| **Default Deny** | 정책이 없으면 모두 거부 | `policyEnforcementMode: always` |

> **⚠️ 주의 (Warning)**: 프로덕션 환경에서는 점진적으로 정책을 적용하세요. 갑자기 모든 트래픽을 차단하면 서비스 장애가 발생할 수 있습니다.

### 섹션 요약

| 정책 레벨 | 제어 가능 항목 | 성능 영향 |
|-----------|---------------|-----------|
| **L3** | IP, CIDR, Label | 최소 |
| **L4** | TCP/UDP 포트 | 최소 |
| **L7** | HTTP, gRPC, Kafka | 있음 (Envoy) |
| **DNS** | FQDN 기반 | 있음 (DNS 모니터링) |

---

## 9. Hubble 관찰성

Hubble은 Cilium의 관찰성(Observability) 계층으로, eBPF를 활용하여 네트워크 흐름을 실시간으로 모니터링합니다.

### 9.1 Hubble 아키텍처

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                            Hubble 아키텍처                                           │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                     │
│   ┌─────────────────────────────────────────────────────────────────────────────┐   │
│   │                              Hubble UI                                       │   │
│   │                        (Web Dashboard)                                       │   │
│   │                                                                              │   │
│   │  • Service Map 시각화                                                        │   │
│   │  • 실시간 흐름 모니터링                                                       │   │
│   │  • 정책 위반 표시                                                            │   │
│   │                                                                              │   │
│   └──────────────────────────────────────┬──────────────────────────────────────┘   │
│                                          │                                          │
│                                          ▼                                          │
│   ┌─────────────────────────────────────────────────────────────────────────────┐   │
│   │                           Hubble Relay                                       │   │
│   │                      (Deployment - 클러스터 전체)                             │   │
│   │                                                                              │   │
│   │  • 모든 노드의 Hubble 데이터 집계                                            │   │
│   │  • gRPC API 제공                                                             │   │
│   │  • hubble CLI 연결 포인트                                                    │   │
│   │                                                                              │   │
│   └────────────────────────┬───────────────────────┬────────────────────────────┘   │
│                            │                       │                                │
│              ┌─────────────┴─────────────┐ ┌───────┴───────────────┐               │
│              ▼                           ▼ ▼                       ▼               │
│   ┌─────────────────────┐     ┌─────────────────────┐    ┌─────────────────────┐   │
│   │     Node 1          │     │     Node 2          │    │     Node 3          │   │
│   │                     │     │                     │    │                     │   │
│   │  ┌───────────────┐  │     │  ┌───────────────┐  │    │  ┌───────────────┐  │   │
│   │  │ Cilium Agent  │  │     │  │ Cilium Agent  │  │    │  │ Cilium Agent  │  │   │
│   │  │               │  │     │  │               │  │    │  │               │  │   │
│   │  │  Hubble       │  │     │  │  Hubble       │  │    │  │  Hubble       │  │   │
│   │  │  (embedded)   │  │     │  │  (embedded)   │  │    │  │  (embedded)   │  │   │
│   │  │               │  │     │  │               │  │    │  │               │  │   │
│   │  │  ┌─────────┐  │  │     │  │  ┌─────────┐  │  │    │  │  ┌─────────┐  │  │   │
│   │  │  │ eBPF    │  │  │     │  │  │ eBPF    │  │  │    │  │  │ eBPF    │  │  │   │
│   │  │  │ events  │  │  │     │  │  │ events  │  │  │    │  │  │ events  │  │  │   │
│   │  │  └─────────┘  │  │     │  │  └─────────┘  │  │    │  │  └─────────┘  │  │   │
│   │  └───────────────┘  │     │  └───────────────┘  │    │  └───────────────┘  │   │
│   │                     │     │                     │    │                     │   │
│   │  ┌───────────────┐  │     │  ┌───────────────┐  │    │  ┌───────────────┐  │   │
│   │  │    Pods       │  │     │  │    Pods       │  │    │  │    Pods       │  │   │
│   │  └───────────────┘  │     │  └───────────────┘  │    │  └───────────────┘  │   │
│   │                     │     │                     │    │                     │   │
│   └─────────────────────┘     └─────────────────────┘    └─────────────────────┘   │
│                                                                                     │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

### 9.2 Hubble 설정

```yaml
# cilium-values-custom.yaml
hubble:
  # Hubble 활성화
  enabled: true

  # Hubble 메트릭 설정
  metrics:
    # 수집할 메트릭 목록
    enabled:
    - dns           # DNS 쿼리 메트릭
    - drop          # 드롭된 패킷 메트릭
    - tcp           # TCP 연결 메트릭
    - flow          # 네트워크 플로우 메트릭
    - port-distribution  # 포트 분포 메트릭
    - icmp          # ICMP 메트릭
    - 'httpV2:exemplars=true;labelsContext=source_ip,source_namespace,source_workload,destination_ip,destination_namespace,destination_workload,traffic_direction'
    
    # OpenMetrics 형식으로 내보내기
    enableOpenMetrics: true
    port: 9965

  # Hubble Relay (데이터 집계)
  relay:
    enabled: true
    rollOutPods: true
    prometheus:
      enabled: true
      port: 9966

  # Hubble UI (웹 대시보드)
  ui:
    enabled: true
    rollOutPods: true
    service:
      type: NodePort
      nodePort: 31235
    ingress:
      enabled: true
      className: "cilium"
      hosts:
      - hubble-ui.cilium.io
```

### 9.3 Hubble CLI 사용법

```bash
# Hubble CLI 설치 (이미 오프라인 준비됨)
$ hubble version
hubble v1.18.5

# Hubble Relay에 연결 (포트 포워딩)
$ kubectl port-forward -n kube-system svc/hubble-relay 4245:80 &
$ hubble status
Healthcheck (via localhost:4245): Ok
Current/Max Flows: 8,190/8,190 (100.00%)
Flows/s: 45.32

# 실시간 흐름 모니터링
$ hubble observe
Jan 20 05:03:15.123: 10.200.1.45:32456 -> 10.201.0.50:80 http-request FORWARDED (HTTP/1.1 GET /api/health)
Jan 20 05:03:15.124: 10.201.0.50:80 -> 10.200.1.45:32456 http-response FORWARDED (HTTP/1.1 200 0ms)
Jan 20 05:03:15.456: 10.200.2.67:45123 -> 10.200.3.89:6379 to-endpoint FORWARDED (TCP Flags: SYN)

# 특정 Pod 트래픽만 필터링
$ hubble observe --pod production/frontend-abc123
$ hubble observe --namespace production

# 드롭된 패킷만 확인
$ hubble observe --verdict DROPPED
Jan 20 05:03:20.789: 10.200.1.45:54321 -> 10.200.2.67:22 Policy denied DROPPED

# DNS 쿼리 확인
$ hubble observe --protocol dns
Jan 20 05:03:21.000: 10.200.1.45:32145 -> 10.201.0.10:53 dns-request FORWARDED (DNS Query api.example.com A)
Jan 20 05:03:21.005: 10.201.0.10:53 -> 10.200.1.45:32145 dns-response FORWARDED (DNS Answer 93.184.216.34)

# JSON 출력 (파이프라인 처리용)
$ hubble observe -o json | jq '.flow.source.namespace'
```

> **💡 팁 (Tip)**: `hubble observe --follow`로 실시간 스트리밍을 할 수 있습니다. `--last 100`으로 최근 100개 이벤트를 확인할 수 있습니다.

### 9.4 Hubble UI

Hubble UI는 웹 기반 대시보드로 서비스 맵과 네트워크 흐름을 시각화합니다.

```bash
# Hubble UI 접속
# NodePort: http://<node-ip>:31235
# Ingress: http://hubble-ui.cilium.io (DNS 설정 필요)

# 또는 포트 포워딩
$ kubectl port-forward -n kube-system svc/hubble-ui 12000:80 &
$ open http://localhost:12000
```

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         Hubble UI 주요 기능                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   1. Service Map (서비스 맵)                                                 │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                                                                      │   │
│   │        ┌──────────┐         ┌──────────┐         ┌──────────┐       │   │
│   │        │ frontend │────────►│ backend  │────────►│  redis   │       │   │
│   │        │  (3/3)   │   HTTP  │  (5/5)   │   TCP   │  (1/1)   │       │   │
│   │        └──────────┘         └──────────┘         └──────────┘       │   │
│   │             │                     │                                  │   │
│   │             │                     │ HTTP                            │   │
│   │             ▼                     ▼                                  │   │
│   │        ┌──────────┐         ┌──────────┐                            │   │
│   │        │ external │         │ postgres │                            │   │
│   │        │  (world) │         │  (1/1)   │                            │   │
│   │        └──────────┘         └──────────┘                            │   │
│   │                                                                      │   │
│   │   • 화살표: 트래픽 방향                                              │   │
│   │   • 녹색: 정상 연결                                                  │   │
│   │   • 빨간색: 정책 위반/드롭                                           │   │
│   │   • 숫자: Ready/Total Pod 수                                        │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   2. Flows (흐름 목록)                                                       │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                                                                      │   │
│   │   Time     | Source           | Destination     | Verdict | Info    │   │
│   │   ─────────┼──────────────────┼─────────────────┼─────────┼──────── │   │
│   │   05:03:15 | frontend-abc     | backend-xyz     | FORWARDED| HTTP    │   │
│   │   05:03:15 | backend-xyz      | redis-123       | FORWARDED| TCP     │   │
│   │   05:03:20 | unknown-pod      | postgres-456    | DROPPED  | Policy  │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│   3. Namespace Filter (네임스페이스 필터)                                    │
│      - 특정 네임스페이스만 선택하여 보기                                     │
│      - 클러스터 전체 또는 특정 서비스에 집중                                 │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 9.5 Prometheus 메트릭 통합

```bash
# Hubble 메트릭 확인
$ kubectl port-forward -n kube-system ds/cilium 9965:9965 &
$ curl -s localhost:9965/metrics | grep hubble

# 주요 메트릭
hubble_flows_processed_total{protocol="TCP",subtype="to-endpoint",type="Trace",verdict="FORWARDED"} 12345
hubble_dns_queries_total{rcode="No Error",source="frontend-abc"} 567
hubble_drop_total{reason="POLICY_DENIED"} 89
```

### 섹션 요약

| 컴포넌트 | 역할 | 접근 방법 |
|----------|------|-----------|
| **Hubble (embedded)** | 노드별 이벤트 수집 | Agent 내장 |
| **Hubble Relay** | 클러스터 데이터 집계 | gRPC API |
| **Hubble CLI** | 커맨드라인 분석 | `hubble observe` |
| **Hubble UI** | 웹 시각화 | 브라우저 |

---

## 이 장의 요약

| 핵심 개념 | 설명 | 중요도 |
|-----------|------|--------|
| **eBPF** | 커널에서 안전하게 실행되는 프로그램, 네트워킹/보안/관찰성의 기반 | ⭐⭐⭐⭐⭐ |
| **BPF Verifier** | 프로그램 안전성 검증, 커널 보호자 | ⭐⭐⭐⭐⭐ |
| **BPF Maps** | 커널/사용자 공간 데이터 공유, 상태 저장 | ⭐⭐⭐⭐⭐ |
| **XDP** | 드라이버 레벨 패킷 처리, 최고 성능 | ⭐⭐⭐⭐ |
| **TC Hook** | sk_buff 레벨 패킷 처리, 풍부한 기능 | ⭐⭐⭐⭐ |
| **Socket LB** | connect() 시점 로드밸런싱, NAT 우회 | ⭐⭐⭐⭐⭐ |
| **Cilium Agent** | 노드별 eBPF 데이터플레인 관리 | ⭐⭐⭐⭐⭐ |
| **Cilium Operator** | 클러스터 수준 IPAM, Identity 관리 | ⭐⭐⭐⭐ |
| **Security Identity** | 라벨 기반 숫자 ID, 빠른 정책 결정 | ⭐⭐⭐⭐ |
| **kube-proxy 대체** | iptables O(n) → eBPF O(1), 성능 혁신 | ⭐⭐⭐⭐⭐ |
| **Maglev 해싱** | 일관된 해싱, Backend 변경 시 최소 재분배 | ⭐⭐⭐⭐ |
| **DSR 모드** | 응답 트래픽 직접 반환, LB 부하 감소 | ⭐⭐⭐ |
| **netkit 모드** | L3 데이터패스, veth 대비 10-15% 성능 향상 | ⭐⭐⭐⭐ |
| **CiliumNetworkPolicy** | L3/L4/L7 네트워크 정책, DNS FQDN 지원 | ⭐⭐⭐⭐⭐ |
| **Hubble** | eBPF 기반 네트워크 관찰성, 실시간 모니터링 | ⭐⭐⭐⭐ |

---

## 실습 과제

### 실습 1: eBPF 프로그램 탐색

**목표**: 시스템에 로드된 eBPF 프로그램과 맵을 탐색합니다.

```bash
# 1. bpftool 설치 확인
$ which bpftool
$ bpftool version

# 2. 로드된 eBPF 프로그램 목록 확인
$ bpftool prog list

# 3. Cilium 관련 프로그램 필터링
$ bpftool prog list | grep -i cilium

# 4. 특정 프로그램 상세 정보 확인 (ID를 위 결과에서 확인)
$ bpftool prog show id <ID>

# 5. BPF 맵 목록 확인
$ bpftool map list | grep -i cilium

# 6. 특정 맵 내용 덤프 (예: cilium_lb4_services)
$ bpftool map dump name cilium_lb4_services | head -20
```

**예상 결과**:
- Cilium이 로드한 다양한 eBPF 프로그램 확인 (XDP, TC, Socket 등)
- 서비스 맵, 연결 추적 맵, 정책 맵 등 확인

---

### 실습 2: Cilium CLI로 서비스 로드밸런싱 확인

**목표**: Cilium의 eBPF 기반 서비스 로드밸런싱을 확인합니다.

```bash
# 1. Cilium 상태 확인
$ cilium status

# 2. kube-proxy 대체 모드 확인
$ cilium status | grep -A5 KubeProxyReplacement

# 3. 서비스 맵 확인 (eBPF 맵 기반)
$ cilium bpf lb list

# 4. 특정 서비스의 백엔드 확인
$ cilium bpf lb list | grep <service-ip>

# 5. Endpoint 목록 확인
$ cilium endpoint list

# 6. 특정 Endpoint의 정책 상태 확인
$ cilium endpoint get <endpoint-id>
```

**예상 결과**:
- KubeProxyReplacement: True 확인
- 모든 Kubernetes Service가 eBPF 맵에 등록됨
- 각 Service에 대한 Backend Pod IP 확인

---

### 실습 3: Hubble로 네트워크 흐름 분석

**목표**: Hubble을 사용하여 실시간 네트워크 트래픽을 관찰합니다.

```bash
# 1. Hubble Relay 연결 (포트 포워딩)
$ kubectl port-forward -n kube-system svc/hubble-relay 4245:80 &

# 2. Hubble 상태 확인
$ hubble status

# 3. 모든 흐름 실시간 관찰
$ hubble observe --follow

# 4. 특정 네임스페이스만 필터링
$ hubble observe --namespace kube-system

# 5. 드롭된 패킷만 확인
$ hubble observe --verdict DROPPED

# 6. DNS 쿼리만 확인
$ hubble observe --protocol dns

# 7. HTTP 트래픽 확인 (L7 정책이 적용된 경우)
$ hubble observe --protocol http

# 8. JSON 출력으로 상세 분석
$ hubble observe --last 10 -o json | jq '.flow.destination'
```

**예상 결과**:
- Pod 간 실시간 트래픽 확인
- 정책으로 인해 드롭된 패킷 식별
- DNS 쿼리와 응답 추적

---

### 실습 4: CiliumNetworkPolicy 적용 및 테스트

**목표**: L3/L4 네트워크 정책을 적용하고 효과를 확인합니다.

```bash
# 1. 테스트 네임스페이스 생성
$ kubectl create namespace policy-test

# 2. 테스트 Pod 배포
$ kubectl run -n policy-test web --image=nginx:alpine --labels="app=web"
$ kubectl run -n policy-test client --image=busybox --command -- sleep 3600

# 3. 정책 적용 전 연결 테스트
$ kubectl exec -n policy-test client -- wget -q -O- --timeout=3 web

# 4. 모든 Ingress 트래픽 차단 정책 적용
$ cat << 'EOF' | kubectl apply -f -
apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
metadata:
  name: "deny-all-ingress"
  namespace: policy-test
spec:
  endpointSelector:
    matchLabels:
      app: web
  ingress: []
EOF

# 5. 정책 적용 후 연결 테스트 (실패해야 함)
$ kubectl exec -n policy-test client -- wget -q -O- --timeout=3 web

# 6. Hubble로 드롭 확인
$ hubble observe --namespace policy-test --verdict DROPPED

# 7. 특정 라벨의 Pod만 허용하도록 정책 수정
$ cat << 'EOF' | kubectl apply -f -
apiVersion: "cilium.io/v2"
kind: CiliumNetworkPolicy
metadata:
  name: "allow-client-only"
  namespace: policy-test
spec:
  endpointSelector:
    matchLabels:
      app: web
  ingress:
    - fromEndpoints:
        - matchLabels:
            run: client
      toPorts:
        - ports:
            - port: "80"
              protocol: TCP
EOF

# 8. 다시 연결 테스트 (성공해야 함)
$ kubectl exec -n policy-test client -- wget -q -O- --timeout=3 web

# 9. 정리
$ kubectl delete namespace policy-test
```

**예상 결과**:
- 정책 없음 → 연결 성공
- deny-all → 연결 실패, Hubble에서 DROPPED 확인
- allow-client-only → 연결 성공

---

### 실습 5: JIT 컴파일 및 성능 확인

**목표**: eBPF JIT 컴파일러 상태를 확인하고 성능 영향을 이해합니다.

```bash
# 1. JIT 활성화 상태 확인
$ cat /proc/sys/net/core/bpf_jit_enable

# 2. JIT 하드닝 상태 확인
$ cat /proc/sys/net/core/bpf_jit_harden

# 3. eBPF 프로그램의 JIT 컴파일 상태 확인
$ bpftool prog list | grep jited
# jited: 바이트 수가 0보다 크면 JIT 컴파일됨

# 4. 특정 프로그램의 JIT 어셈블리 확인 (고급)
$ bpftool prog dump jited id <ID> | head -30

# 5. eBPF 프로그램 통계 확인
$ bpftool prog show id <ID> --json | jq '.run_cnt, .run_time_ns'

# 6. Cilium 데이터패스 모드 확인
$ cilium status --verbose | grep -i datapath
```

**예상 결과**:
- `bpf_jit_enable = 1` (활성화)
- 모든 Cilium 프로그램이 JIT 컴파일됨
- 프로그램 실행 통계 확인

---

## 학습 점검

### 질문 1: eBPF의 핵심 장점

**Q**: eBPF가 커널 모듈 대비 가지는 핵심 장점 3가지는 무엇인가요?

<details>
<summary>정답 보기</summary>

1. **안전성**: BPF Verifier가 모든 프로그램을 검증하여 커널 크래시 방지
2. **재시작 불필요**: 커널 재부팅 없이 프로그램 로드/언로드 가능
3. **권한 분리**: CAP_BPF 등 세분화된 권한으로 보안성 향상
4. (추가) **이식성**: BTF/CO-RE로 커널 버전 간 호환성 확보

</details>

---

### 질문 2: BPF Verifier의 역할

**Q**: BPF Verifier가 프로그램을 거부하는 일반적인 이유 3가지를 설명하세요.

<details>
<summary>정답 보기</summary>

1. **무한 루프**: 종료가 보장되지 않는 루프 (bounded loops만 허용)
2. **메모리 접근 위반**: 경계 검사 없이 포인터 역참조 (예: 패킷 데이터 접근 전 bounds check 필요)
3. **초기화되지 않은 변수**: 읽기 전에 초기화되지 않은 레지스터/스택 변수 사용
4. (추가) **프로그램 크기 초과**: 1M instructions 한도 초과
5. (추가) **잘못된 Helper 함수 사용**: 프로그램 타입에서 허용되지 않는 Helper 호출

</details>

---

### 질문 3: BPF Maps 타입

**Q**: Cilium이 서비스 로드밸런싱에 사용하는 BPF Map 타입은 무엇이며, 왜 그 타입을 사용하나요?

<details>
<summary>정답 보기</summary>

**BPF_MAP_TYPE_HASH**를 사용합니다.

이유:
1. **O(1) 룩업**: Service IP:Port를 키로 사용하여 해시 기반 상수 시간 조회
2. **동적 크기**: 서비스가 추가/삭제될 때 유연하게 대응
3. **키-값 구조**: Service VIP를 키로, Backend 정보를 값으로 저장하기에 적합

NAT 테이블에는 **BPF_MAP_TYPE_LRU_HASH**를 사용하여 오래된 엔트리를 자동 퇴거합니다.

</details>

---

### 질문 4: XDP vs TC Hook

**Q**: XDP와 TC ingress hook의 차이점을 설명하고, 각각 어떤 경우에 사용하면 좋은지 설명하세요.

<details>
<summary>정답 보기</summary>

| 항목 | XDP | TC Ingress |
|------|-----|------------|
| **위치** | 드라이버 레벨 (sk_buff 생성 전) | 네트워크 스택 (sk_buff 생성 후) |
| **성능** | 최고 (Zero-Copy 가능) | 좋음 |
| **기능** | 제한적 (L2/L3 헤더만) | 풍부 (sk_buff 전체 접근) |
| **사용 사례** | DDoS 방어, 고성능 LB | 정책 적용, NAT, 복잡한 패킷 수정 |

**XDP 권장**: 단순하고 빠른 결정 필요 (DROP/PASS/REDIRECT)
**TC 권장**: 복잡한 패킷 처리, 상태 기반 결정, L4+ 정보 필요

</details>

---

### 질문 5: Socket LB의 원리

**Q**: Cilium의 Socket-based Load Balancing이 기존 DNAT 방식 대비 가지는 장점을 설명하세요.

<details>
<summary>정답 보기</summary>

**Socket LB 원리**:
- `cgroup/connect4` hook에서 `connect()` 시스템 콜을 가로챔
- Service IP:Port를 직접 Backend Pod IP:Port로 변환
- 애플리케이션은 이미 변환된 주소로 TCP handshake 수행

**장점**:
1. **NAT 불필요**: 패킷 전송 전에 주소 변환 완료, DNAT/SNAT 오버헤드 없음
2. **conntrack 최소화**: NAT가 없으므로 연결 추적 엔트리 감소
3. **직접 연결**: Backend와 직접 TCP 연결, 중간 홉 없음
4. **성능 향상**: 패킷 경로 단축, 지연 시간 감소

</details>

---

### 질문 6: kube-proxy 성능 문제

**Q**: kube-proxy의 iptables 모드가 대규모 클러스터에서 성능 문제를 일으키는 이유를 설명하세요.

<details>
<summary>정답 보기</summary>

1. **O(n) 선형 탐색**: 
   - iptables 규칙은 순차적으로 평가
   - 10,000개 서비스 = ~50,000 규칙
   - 패킷당 평균 25,000개 규칙 검사

2. **규칙 업데이트 비용**:
   - Service/Endpoint 변경 시 전체 규칙 재작성
   - 대규모에서 수 초 소요
   - 업데이트 중 패킷 손실 가능

3. **conntrack 테이블 한계**:
   - 모든 NAT 연결 추적 필요
   - 기본 262,144 엔트리 한계
   - 오버플로우 시 새 연결 실패

**Cilium 해결**:
- BPF Hash Map으로 O(1) 룩업
- Map entry만 개별 업데이트
- Socket LB로 conntrack 최소화

</details>

---

### 질문 7: Security Identity

**Q**: Cilium의 Security Identity가 무엇이며, 왜 라벨 대신 숫자 ID를 사용하는지 설명하세요.

<details>
<summary>정답 보기</summary>

**Security Identity**:
- 동일한 Kubernetes 라벨 집합을 가진 Pod 그룹에 부여되는 고유 숫자 ID
- 예: `app=frontend, tier=web` → Identity 54321

**숫자 ID 사용 이유**:
1. **eBPF 효율성**: 숫자 비교는 문자열 비교보다 훨씬 빠름
2. **BPF Map 키 크기**: 고정 크기 숫자가 가변 길이 라벨보다 효율적
3. **O(1) 룩업**: `cilium_ipcache` 맵에서 IP → Identity 즉시 조회
4. **정책 결정 속도**: 패킷 처리 시 숫자 비교로 빠른 정책 결정

Identity는 `cilium_ipcache` BPF Map을 통해 IP 주소와 매핑됩니다.

</details>

---

### 질문 8: Datapath 모드

**Q**: Cilium의 veth와 netkit 데이터패스 모드의 차이점을 설명하고, netkit 사용 시 요구사항을 명시하세요.

<details>
<summary>정답 보기</summary>

| 항목 | veth | netkit |
|------|------|--------|
| **레이어** | L2 | L3 |
| **BPF 부착** | TC hook | Native BPF |
| **L2 처리** | 필요 | 불필요 |
| **NS 전환** | 기본 | 최적화 |
| **성능** | 기준 | +10-15% |

**netkit 요구사항**:
- Linux 커널 6.7 이상 (RHEL 10은 6.12로 지원)
- `CONFIG_NETKIT=y` 커널 설정

**권장**:
- 커널 6.7+ 환경: **netkit** (성능 향상)
- 레거시 커널: veth (호환성)
- L2 프로토콜 필요: netkit-l2

</details>

---

### 질문 9: Maglev 해싱

**Q**: Maglev 일관된 해싱이 라운드 로빈 대비 가지는 장점을 설명하세요.

<details>
<summary>정답 보기</summary>

**Maglev 장점**:

1. **최소 재분배 (Minimal Disruption)**:
   - Backend 추가/제거 시 영향받는 연결 최소화
   - 라운드 로빈: 모든 매핑 변경
   - Maglev: 변경된 Backend 관련 연결만 재분배

2. **세션 지속성 (Session Affinity)**:
   - 동일 클라이언트(5-tuple)는 동일 Backend로 전달
   - Stateful 애플리케이션에 중요

3. **균등 분배**:
   - 65537(소수) 크기 룩업 테이블로 균등 분배
   - 각 Backend가 동일한 비율로 트래픽 수신

**동작 원리**:
- Hash(5-tuple) mod 65537 → 룩업 테이블 인덱스 → Backend

</details>

---

### 질문 10: Hubble의 역할

**Q**: Hubble이 제공하는 관찰성 기능 3가지와 각각의 사용 사례를 설명하세요.

<details>
<summary>정답 보기</summary>

1. **실시간 흐름 모니터링 (Flow Observability)**:
   - `hubble observe --follow`
   - 사용 사례: 네트워크 문제 디버깅, 트래픽 패턴 분석

2. **정책 위반 탐지 (Policy Verdict)**:
   - `hubble observe --verdict DROPPED`
   - 사용 사례: 정책 오류 확인, 보안 감사

3. **서비스 맵 시각화 (Service Map)**:
   - Hubble UI의 Service Map 뷰
   - 사용 사례: 서비스 의존성 파악, 아키텍처 문서화

4. (추가) **DNS 분석**:
   - `hubble observe --protocol dns`
   - 사용 사례: DNS 기반 정책 검증, FQDN 접근 추적

5. (추가) **메트릭 수집**:
   - Prometheus 통합
   - 사용 사례: 대시보드, 알림, SLO 모니터링

</details>

---

## 다음 단계

### 이 장을 마친 후

1. **실습 환경 구축**: 실제 Kubernetes 클러스터에서 Cilium 배포
2. **정책 설계**: 애플리케이션 요구사항에 맞는 네트워크 정책 작성
3. **모니터링 구축**: Hubble + Prometheus + Grafana 관찰성 스택 구성

### 다음 장 미리보기

**[07장: 오프라인 인프라](07-offline-infrastructure.md)**에서는:
- 폐쇄망(Air-gapped) 환경의 특성과 도전 과제
- 프록시 레지스트리 구성 및 운영
- Nginx 파일 서버를 통한 바이너리 호스팅
- Kubespray 오프라인 워크플로우

---

## 참고 자료

### 공식 문서

| 리소스 | URL |
|--------|-----|
| **eBPF 공식 사이트** | https://ebpf.io |
| **Cilium 문서** | https://docs.cilium.io |
| **Hubble 문서** | https://docs.cilium.io/en/stable/observability/hubble |
| **BPF/eBPF 레퍼런스** | https://www.kernel.org/doc/html/latest/bpf |

### 추천 도서

| 도서 | 저자 | 내용 |
|------|------|------|
| **Learning eBPF** | Liz Rice | eBPF 기초부터 실습까지 |
| **BPF Performance Tools** | Brendan Gregg | 성능 분석 도구 개발 |
| **Linux Observability with BPF** | David Calavera | BPF 기반 관찰성 |

### 유용한 도구

| 도구 | 용도 |
|------|------|
| **bpftool** | eBPF 프로그램/맵 관리 |
| **cilium CLI** | Cilium 상태 확인 및 관리 |
| **hubble CLI** | 네트워크 흐름 분석 |
| **bpftrace** | 동적 eBPF 추적 |

### 프로젝트 내 관련 파일

| 파일 | 설명 |
|------|------|
| `configs/cilium/cilium-values-custom.yaml` | Cilium Helm 값 파일 |
| `docs/day0-preparation/04-cilium-offline-prepare.md` | Cilium 오프라인 준비 |
| `docs/day1-deployment/02-cilium-install.md` | Cilium 설치 가이드 |
| `sources/cilium/` | Cilium CLI, Helm Charts |

---

> **다음 장**: [07. 오프라인 인프라](07-offline-infrastructure.md) - 폐쇄망 환경에서의 Kubernetes 인프라 구축
